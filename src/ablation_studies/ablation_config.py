"""
æ¶ˆèå®éªŒé€šç”¨é…ç½®
"""
import os
import json
import torch
import numpy as np
from pathlib import Path
from torch.utils.data import DataLoader

# å¯¼å…¥æ•°æ®å¤„ç†æ¨¡å—
import sys
sys.path.append(str(Path(__file__).parent.parent.parent))
from src.data import (
    SimpleTokenizer, TranslationDataset, collate_fn,
    load_iwslt14_de_en, load_vocabulary_from_file
)

# åŸºç¡€é…ç½®
BASE_CONFIG = {
    # æ•°æ®å‚æ•°
    'max_vocab_size': 10000,
    'batch_size': 64,
    'max_seq_len': 5000,  # ä¸train.pyä¿æŒä¸€è‡´ï¼ˆTransformeré»˜è®¤å€¼ï¼‰
    'max_samples': 50000,  # ä½¿ç”¨50000æ¡æ•°æ®
    'data_dir': 'datasets/iwslt14',  # æ•°æ®é›†è·¯å¾„
    
    # æ¨¡å‹å‚æ•°ï¼ˆGPUä¼˜åŒ–é…ç½®ï¼‰
    'd_model': 256,  # æ ‡å‡†æ¨¡å‹å¤§å°
    'num_heads': 4,
    'num_encoder_layers': 3,  # æ ‡å‡†å±‚æ•°
    'num_decoder_layers': 3,  # æ ‡å‡†å±‚æ•°
    'd_ff': 1024,  # æ ‡å‡†FFNç»´åº¦
    'dropout': 0.1,
    
    # è®­ç»ƒå‚æ•°
    'num_epochs': 10,  # ç»Ÿä¸€ä½¿ç”¨10ä¸ªepoch
    'learning_rate': 0.0001,
    
    # å…¶ä»–
    'device': 'cuda:0' if torch.cuda.is_available() else 'cpu',
    'seed': 42,
    'num_runs': 1,  # æ¯ä¸ªé…ç½®è¿è¡Œæ¬¡æ•°ï¼ˆå¯è®¾ä¸º3å–å¹³å‡ï¼‰
}

# ç»“æœä¿å­˜è·¯å¾„
ABLATION_RESULTS_DIR = Path('results')
ABLATION_RESULTS_DIR.mkdir(parents=True, exist_ok=True)


def set_seed(seed):
    """è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯å¤ç°æ€§"""
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)


def prepare_data(max_vocab_size=10000, batch_size=64, max_samples=50000, data_dir='datasets/iwslt14'):
    """
    å‡†å¤‡è®­ç»ƒå’ŒéªŒè¯æ•°æ®ï¼ˆä½¿ç”¨çœŸå®IWSLT14æ•°æ®é›†ï¼‰
    
    Args:
        max_vocab_size: æœ€å¤§è¯æ±‡è¡¨å¤§å°
        batch_size: æ‰¹é‡å¤§å°
        max_samples: æœ€å¤§è®­ç»ƒæ ·æœ¬æ•°ï¼ˆNoneè¡¨ç¤ºä½¿ç”¨å…¨éƒ¨ï¼‰
        data_dir: æ•°æ®é›†ç›®å½•
    
    Returns:
        train_loader, val_loader, src_vocab_size, tgt_vocab_size, pad_idx
    """
    print(f"ğŸ“¦ åŠ è½½IWSLT14æ•°æ®é›†...")
    print(f"  æ•°æ®ç›®å½•: {data_dir}")
    print(f"  æœ€å¤§æ ·æœ¬æ•°: {max_samples if max_samples else 'å…¨éƒ¨'}")
    
    # åˆ›å»ºåˆ†è¯å™¨
    tokenizer = SimpleTokenizer(lowercase=False)
    
    # åŠ è½½çœŸå®æ•°æ®é›†
    train_src, train_tgt, val_src, val_tgt = load_iwslt14_de_en(
        data_dir=data_dir,
        max_samples=max_samples
    )
    
    print(f"  è®­ç»ƒé›†å¤§å°: {len(train_src)}")
    print(f"  éªŒè¯é›†å¤§å°: {len(val_src)}")
    
    # åŠ è½½è¯æ±‡è¡¨
    src_vocab = load_vocabulary_from_file(f"{data_dir}/vocab.de")
    tgt_vocab = load_vocabulary_from_file(f"{data_dir}/vocab.en")
    
    print(f"  æºè¯æ±‡è¡¨å¤§å°: {len(src_vocab)}")
    print(f"  ç›®æ ‡è¯æ±‡è¡¨å¤§å°: {len(tgt_vocab)}")
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = TranslationDataset(train_src, train_tgt, src_vocab, tgt_vocab, tokenizer)
    val_dataset = TranslationDataset(val_src, val_tgt, src_vocab, tgt_vocab, tokenizer)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        collate_fn=lambda batch: collate_fn(batch, pad_idx=src_vocab.pad_idx),
        num_workers=2,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        collate_fn=lambda batch: collate_fn(batch, pad_idx=src_vocab.pad_idx),
        num_workers=2,
        pin_memory=True
    )
    
    return train_loader, val_loader, len(src_vocab), len(tgt_vocab), src_vocab.pad_idx
    

def save_results(experiment_name, results):
    """ä¿å­˜å®éªŒç»“æœåˆ°JSONæ–‡ä»¶"""
    save_dir = ABLATION_RESULTS_DIR / experiment_name
    save_dir.mkdir(parents=True, exist_ok=True)
    
    results_file = save_dir / 'results.json'
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    return save_dir


def load_results(experiment_name):
    """åŠ è½½å·²ä¿å­˜çš„å®éªŒç»“æœ"""
    results_file = ABLATION_RESULTS_DIR / experiment_name / 'results.json'
    if results_file.exists():
        with open(results_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    return None


def print_experiment_header(experiment_name, description):
    """æ‰“å°å®éªŒæ ‡é¢˜"""
    print("\n" + "="*70)
    print(f"  æ¶ˆèå®éªŒ: {experiment_name}")
    print("="*70)
    print(f"æè¿°: {description}")
    print(f"è®¾å¤‡: {BASE_CONFIG['device']}")
    print(f"åŸºç¡€é…ç½®: d_model={BASE_CONFIG['d_model']}, "
          f"epochs={BASE_CONFIG['num_epochs']}, "
          f"max_samples={BASE_CONFIG['max_samples']}")
    print("="*70 + "\n")


def print_experiment_summary(results):
    """æ‰“å°å®éªŒç»“æœæ‘˜è¦"""
    print("\n" + "="*70)
    print("  å®éªŒç»“æœæ‘˜è¦")
    print("="*70)
    
    for config_name, metrics in results.items():
        print(f"\nã€{config_name}ã€‘")
        if isinstance(metrics, dict):
            for key, value in metrics.items():
                if isinstance(value, (int, float)):
                    print(f"  {key}: {value:.4f}")
                elif isinstance(value, list) and len(value) > 0:
                    if isinstance(value[-1], (int, float)):
                        print(f"  {key} (æœ€ç»ˆ): {value[-1]:.4f}")
        else:
            print(f"  ç»“æœ: {metrics}")
    
    print("="*70 + "\n")


# å¯è§†åŒ–æ ·å¼é…ç½®
PLOT_CONFIG = {
    'figure_size': (7, 3.5),  # 7è‹±å¯¸å®½åº¦
    'dpi': 150,
    'line_width': 2,
    'marker_size': 6,
    'font_size': 10,  # 10ptå­—ä½“
    'title_size': 12,
    'legend_size': 9,
    'colors': ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', 
               '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'],
}

