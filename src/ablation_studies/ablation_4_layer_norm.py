"""
æ¶ˆèå®éªŒ4: å±‚å½’ä¸€åŒ–ç­–ç•¥ (Layer Normalization Strategy)

æµ‹è¯•ä¸åŒå±‚å½’ä¸€åŒ–ç­–ç•¥å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼š
1. Post-LN (åŸå§‹Transformer)
2. Pre-LN (æ›´ç¨³å®šçš„è®­ç»ƒ)
3. æ— LayerNorm
"""
import sys
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import math  # ä½¿ç”¨math.expè®¡ç®—å›°æƒ‘åº¦ï¼Œä¸train.pyä¿æŒä¸€è‡´
import numpy as np  # ç”¨äºisnanã€isinfç­‰è¾…åŠ©å‡½æ•°
import argparse
from pathlib import Path

sys.path.append(str(Path(__file__).parent.parent.parent))

from src.ablation_studies.ablation_config import (
    BASE_CONFIG, set_seed, save_results, prepare_data, 
    print_experiment_header, print_experiment_summary, PLOT_CONFIG
)
from src.transformer import Transformer
# from src.data.dataset import create_dataloaders
from train import train_epoch, evaluate

plt.rcParams['font.sans-serif'] = ['Noto Sans CJK JP', 'Droid Sans Fallback', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


def modify_transformer_layer_norm(model, ln_type='post'):
    """
    ä¿®æ”¹Transformerçš„LayerNormç­–ç•¥
    
    Args:
        model: Transformeræ¨¡å‹
        ln_type: 'post' (Post-LN), 'pre' (Pre-LN), 'none' (æ— LN)
    """
    if ln_type == 'pre':
        # Pre-LN: å°†LayerNormç§»åˆ°å­å±‚ä¹‹å‰
        # è¿™éœ€è¦ä¿®æ”¹æ¨¡å‹ç»“æ„ï¼Œè¿™é‡Œæˆ‘ä»¬é€šè¿‡ä¿®æ”¹å‰å‘ä¼ æ’­é€»è¾‘æ¥æ¨¡æ‹Ÿ
        pass  # æ¨¡å‹å·²ç»æ”¯æŒpre_normå‚æ•°
    elif ln_type == 'none':
        # ç§»é™¤æ‰€æœ‰LayerNorm
        for module in model.modules():
            if isinstance(module, nn.LayerNorm):
                # å°†LayerNormæ›¿æ¢ä¸ºæ’ç­‰æ˜ å°„
                module.weight.data.fill_(1.0)
                module.bias.data.fill_(0.0)
                module.eval()  # å†»ç»“å‚æ•°
    # 'post' ä½¿ç”¨é»˜è®¤é…ç½®
    return model


def run_ablation_layer_norm(gpu_id=None):
    """è¿è¡Œå±‚å½’ä¸€åŒ–ç­–ç•¥æ¶ˆèå®éªŒ"""
    
    experiment_name = "ablation_4_layer_norm"
    description = "è¯„ä¼°ä¸åŒå±‚å½’ä¸€åŒ–ç­–ç•¥(Post-LN/Pre-LN/æ— LN)å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“"
    
    # å¦‚æœæŒ‡å®šäº†GPUï¼Œè¦†ç›–é…ç½®
    if gpu_id is not None:
        BASE_CONFIG['device'] = f'cuda:{gpu_id}' if torch.cuda.is_available() else 'cpu'
        print(f"ğŸ¯ ä½¿ç”¨GPU: {BASE_CONFIG['device']}")
    
    print_experiment_header(experiment_name, description)
    
    # æµ‹è¯•çš„LayerNormç­–ç•¥
    ln_strategies = {
        'post': 'Post-LNï¼ˆæ ‡å‡†ï¼‰',
        'pre': 'Pre-LNï¼ˆç¨³å®šï¼‰',
        'none': 'æ— LayerNorm'
    }
    
    # å‡†å¤‡æ•°æ®ï¼ˆä½¿ç”¨çœŸå®IWSLT14æ•°æ®é›†ï¼‰
    train_loader, val_loader, src_vocab_size, tgt_vocab_size, pad_idx = prepare_data(
        max_vocab_size=BASE_CONFIG['max_vocab_size'],
        batch_size=BASE_CONFIG['batch_size'],
        max_samples=BASE_CONFIG['max_samples'],
        data_dir=BASE_CONFIG['data_dir']
    )
    
    # æŸå¤±å‡½æ•°ï¼ˆä½¿ç”¨ä¸train.pyä¸€è‡´çš„LabelSmoothingï¼‰
    from train import LabelSmoothing
    criterion = LabelSmoothing(
        vocab_size=tgt_vocab_size,
        pad_idx=pad_idx,
        smoothing=0.05
    )
    
    all_results = {}
    all_history = {}
    
    for ln_type, ln_name in ln_strategies.items():
        print(f"\n{'='*60}")
        print(f"  æµ‹è¯•é…ç½®: {ln_name}")
        print(f"{'='*60}\n")
        
        set_seed(BASE_CONFIG['seed'])
        
        # åˆ›å»ºæ¨¡å‹
        model = Transformer(
            src_vocab_size=src_vocab_size,
            tgt_vocab_size=tgt_vocab_size,
            d_model=BASE_CONFIG['d_model'],
            num_heads=BASE_CONFIG['num_heads'],
            num_encoder_layers=BASE_CONFIG['num_encoder_layers'],
            num_decoder_layers=BASE_CONFIG['num_decoder_layers'],
            d_ff=BASE_CONFIG['d_ff'],
            dropout=BASE_CONFIG['dropout'],
            max_seq_len=BASE_CONFIG['max_seq_len'],
            pad_idx=pad_idx  # æ·»åŠ pad_idxå‚æ•°
        )
        
        # ä¿®æ”¹LayerNormç­–ç•¥
        model = modify_transformer_layer_norm(model, ln_type)
        model = model.to(BASE_CONFIG['device'])
        
        num_params = sum(p.numel() for p in model.parameters())
        print(f"æ¨¡å‹å‚æ•°é‡: {num_params:,}")
        
        # ä¼˜åŒ–å™¨ï¼ˆä½¿ç”¨ä¸train.pyä¸€è‡´çš„è®ºæ–‡é…ç½®ï¼‰
        optimizer = torch.optim.Adam(
            model.parameters(), 
            lr=BASE_CONFIG['learning_rate'],
            betas=(0.9, 0.98),
            eps=1e-9
        )
        
        # è®­ç»ƒ
        train_losses = []
        val_losses = []
        train_ppls = []
        val_ppls = []
        
        print(f"\nå¼€å§‹è®­ç»ƒ ({BASE_CONFIG['num_epochs']} epochs)...")
        
        try:
            for epoch in range(1, BASE_CONFIG['num_epochs'] + 1):
                train_loss = train_epoch(model, train_loader, optimizer, criterion, BASE_CONFIG['device'], pad_idx)
                val_loss = evaluate(model, val_loader, criterion, BASE_CONFIG['device'], pad_idx)
                
                # è®¡ç®—å›°æƒ‘åº¦ï¼ˆä¸train.pyä¿æŒä¸€è‡´ï¼Œæ·»åŠ æº¢å‡ºä¿æŠ¤ï¼‰
                train_ppl = math.exp(train_loss) if train_loss < 10 else float('inf')
                val_ppl = math.exp(val_loss) if val_loss < 10 else float('inf')
                
                train_losses.append(train_loss)
                val_losses.append(val_loss)
                train_ppls.append(train_ppl)
                val_ppls.append(val_ppl)
                
                print(f"Epoch {epoch}/{BASE_CONFIG['num_epochs']}: "
                      f"Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, "
                      f"Val PPL={val_ppl:.2f}")
                
                # æ£€æŸ¥æ˜¯å¦å‡ºç°NaN
                if np.isnan(train_loss) or np.isnan(val_loss):
                    print(f"âš ï¸  æ£€æµ‹åˆ°NaNï¼Œåœæ­¢è®­ç»ƒ")
                    break
                    
        except Exception as e:
            print(f"âŒ è®­ç»ƒå‡ºé”™: {e}")
            # å¦‚æœè®­ç»ƒå¤±è´¥ï¼Œä½¿ç”¨æœ€åæœ‰æ•ˆçš„å€¼
            if not train_losses:
                train_losses = [float('inf')]
                val_losses = [float('inf')]
                train_ppls = [float('inf')]
                val_ppls = [float('inf')]
        
        all_results[ln_name] = {
            'ln_type': ln_type,
            'final_train_loss': train_losses[-1],
            'final_val_loss': val_losses[-1],
            'final_train_ppl': train_ppls[-1],
            'final_val_ppl': val_ppls[-1],
            'num_params': num_params,
            'training_stable': not (np.isnan(train_losses[-1]) or np.isinf(train_losses[-1])),
        }
        all_history[ln_name] = {
            'train_losses': train_losses,
            'val_losses': val_losses,
            'train_ppls': train_ppls,
            'val_ppls': val_ppls,
        }
        
        print(f"âœ… {ln_name} è®­ç»ƒå®Œæˆ\n")
    
    print_experiment_summary(all_results)
    
    # ä¿å­˜ç»“æœ
    results_data = {
        'config': BASE_CONFIG,
        'ln_strategies_tested': ln_strategies,
        'results': all_results,
        'history': all_history,
    }
    save_dir = save_results(experiment_name, results_data)
    
    # ç»˜å›¾
    print("ğŸ“Š ç”Ÿæˆå¯¹æ¯”å›¾è¡¨...")
    plot_comparison(list(ln_strategies.values()), all_results, all_history, save_dir)
    
    print(f"\n{'='*60}")
    print("  å®éªŒå®Œæˆï¼")
    print(f"{'='*60}\n")


def plot_comparison(ln_names, results, history, save_dir):
    """ç»˜åˆ¶å¯¹æ¯”å›¾è¡¨"""
    
    colors = ['#1f77b4', '#ff7f0e', '#d62728']
    
    fig, axes = plt.subplots(2, 2, figsize=(7, 6))
    plt.rcParams.update({'font.size': 10})
    
    # å­å›¾1: è®­ç»ƒæŸå¤±æ›²çº¿
    ax = axes[0, 0]
    for idx, ln_name in enumerate(ln_names):
        losses = history[ln_name]['train_losses']
        # è¿‡æ»¤infå’Œnan
        valid_losses = [l if not (np.isnan(l) or np.isinf(l)) else None for l in losses]
        epochs = range(1, len(losses)+1)
        ax.plot(epochs, valid_losses, 
                label=ln_name, 
                color=colors[idx], linewidth=2.5, marker='o', markersize=6)
    ax.set_xlabel('Epoch', fontsize=12)
    ax.set_ylabel('è®­ç»ƒæŸå¤±', fontsize=12)
    ax.set_title('è®­ç»ƒæŸå¤±æ›²çº¿å¯¹æ¯”', fontsize=14, fontweight='bold')
    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.3)
    ax.set_ylim(bottom=0)
    
    # å­å›¾2: éªŒè¯æŸå¤±æ›²çº¿
    ax = axes[0, 1]
    for idx, ln_name in enumerate(ln_names):
        losses = history[ln_name]['val_losses']
        valid_losses = [l if not (np.isnan(l) or np.isinf(l)) else None for l in losses]
        epochs = range(1, len(losses)+1)
        ax.plot(epochs, valid_losses, 
                label=ln_name, 
                color=colors[idx], linewidth=2.5, marker='s', markersize=6)
    ax.set_xlabel('Epoch', fontsize=12)
    ax.set_ylabel('éªŒè¯æŸå¤±', fontsize=12)
    ax.set_title('éªŒè¯æŸå¤±æ›²çº¿å¯¹æ¯”', fontsize=14, fontweight='bold')
    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.3)
    ax.set_ylim(bottom=0)
    
    # å­å›¾3: æœ€ç»ˆæ€§èƒ½å¯¹æ¯”
    ax = axes[1, 0]
    val_losses = [results[name]['final_val_loss'] for name in ln_names]
    # å¤„ç†infå€¼
    val_losses = [l if not np.isinf(l) else max([v for v in val_losses if not np.isinf(v)], default=1.0) * 2 for l in val_losses]
    bars = ax.bar(ln_names, val_losses, color=colors)
    ax.set_ylabel('æœ€ç»ˆéªŒè¯æŸå¤±', fontsize=12)
    ax.set_title('æœ€ç»ˆéªŒè¯æŸå¤±å¯¹æ¯”', fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3, axis='y')
    plt.setp(ax.xaxis.get_majorticklabels(), rotation=15, ha='right')
    
    for bar, loss in zip(bars, val_losses):
        height = bar.get_height()
        if not np.isinf(loss) and not np.isnan(loss):
            ax.text(bar.get_x() + bar.get_width()/2., height,
                    f'{height:.4f}',
                    ha='center', va='bottom', fontsize=10, fontweight='bold')
        else:
            ax.text(bar.get_x() + bar.get_width()/2., height,
                    'Failed',
                    ha='center', va='bottom', fontsize=10, fontweight='bold', color='red')
    
    # å­å›¾4: è®­ç»ƒç¨³å®šæ€§
    ax = axes[1, 1]
    stability = [1 if results[name]['training_stable'] else 0 for name in ln_names]
    bars = ax.bar(ln_names, stability, color=colors)
    ax.set_ylabel('è®­ç»ƒæ˜¯å¦ç¨³å®š (1=ç¨³å®š, 0=å¤±è´¥)', fontsize=12)
    ax.set_title('è®­ç»ƒç¨³å®šæ€§å¯¹æ¯”', fontsize=14, fontweight='bold')
    ax.set_ylim([0, 1.2])
    ax.grid(True, alpha=0.3, axis='y')
    plt.setp(ax.xaxis.get_majorticklabels(), rotation=15, ha='right')
    
    for bar, stable in zip(bars, stability):
        height = bar.get_height()
        label = 'âœ“ ç¨³å®š' if stable else 'âœ— ä¸ç¨³å®š'
        color = 'green' if stable else 'red'
        ax.text(bar.get_x() + bar.get_width()/2., height + 0.05,
                label, ha='center', va='bottom', fontsize=11, 
                fontweight='bold', color=color)
    
    plt.suptitle('å±‚å½’ä¸€åŒ–ç­–ç•¥æ¶ˆèå®éªŒç»“æœ', fontsize=16, fontweight='bold')
    plt.tight_layout()
    
    save_path = save_dir / 'layer_norm_comparison.png'
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"  âœ… å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜åˆ°: {save_path}")
    plt.close()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='æ¶ˆèå®éªŒ4: å±‚å½’ä¸€åŒ–ç­–ç•¥')
    parser.add_argument('--gpu', type=int, default=None, help='GPU ID (0, 1, 2, 3)')
    args = parser.parse_args()
    
    run_ablation_layer_norm(gpu_id=args.gpu)

