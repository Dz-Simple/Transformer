"""
æ¶ˆèå®éªŒ1: æ³¨æ„åŠ›å¤´æ•° (Number of Attention Heads)

æµ‹è¯•ä¸åŒçš„æ³¨æ„åŠ›å¤´æ•°å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“
"""
import sys
import torch
import matplotlib.pyplot as plt
import math  # ä½¿ç”¨math.expè€Œénp.expï¼Œä¸train.pyä¿æŒä¸€è‡´
import argparse
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent.parent))

from src.ablation_studies.ablation_config import (
    BASE_CONFIG, set_seed, save_results, prepare_data,
    print_experiment_header, print_experiment_summary, PLOT_CONFIG
)
from src.transformer import Transformer
from train import train_epoch, evaluate

plt.rcParams['font.sans-serif'] = ['Noto Sans CJK JP', 'Droid Sans Fallback', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


def run_ablation_num_heads(gpu_id=None):
    """è¿è¡Œæ³¨æ„åŠ›å¤´æ•°æ¶ˆèå®éªŒ"""
    
    experiment_name = "ablation_1_num_heads"
    description = "è¯„ä¼°ä¸åŒæ³¨æ„åŠ›å¤´æ•°(1, 2, 4, 8)å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“"
    
    # å¦‚æœæŒ‡å®šäº†GPUï¼Œè¦†ç›–é…ç½®
    if gpu_id is not None:
        BASE_CONFIG['device'] = f'cuda:{gpu_id}' if torch.cuda.is_available() else 'cpu'
        print(f"ğŸ¯ ä½¿ç”¨GPU: {BASE_CONFIG['device']}")
    
    print_experiment_header(experiment_name, description)
    
    # æµ‹è¯•çš„å¤´æ•°é…ç½®ï¼ˆç¡®ä¿èƒ½è¢«d_modelæ•´é™¤ï¼‰
    num_heads_list = [1, 2, 4, 8]  # å®Œæ•´æµ‹è¯•é…ç½®
    
    # å‡†å¤‡æ•°æ®ï¼ˆä½¿ç”¨çœŸå®IWSLT14æ•°æ®é›†ï¼‰
    train_loader, val_loader, src_vocab_size, tgt_vocab_size, pad_idx = prepare_data(
        max_vocab_size=BASE_CONFIG['max_vocab_size'],
        batch_size=BASE_CONFIG['batch_size'],
        max_samples=BASE_CONFIG['max_samples'],
        data_dir=BASE_CONFIG['data_dir']
    )
    
    # æŸå¤±å‡½æ•°ï¼ˆä½¿ç”¨ä¸train.pyä¸€è‡´çš„LabelSmoothingï¼‰
    from train import LabelSmoothing
    criterion = LabelSmoothing(
        vocab_size=tgt_vocab_size,
        pad_idx=pad_idx,
        smoothing=0.05
    )
    
    # å­˜å‚¨æ‰€æœ‰ç»“æœ
    all_results = {}
    all_history = {}
    
    # å¯¹æ¯ä¸ªå¤´æ•°é…ç½®è¿›è¡Œå®éªŒ
    for num_heads in num_heads_list:
        print(f"\n{'='*60}")
        print(f"  æµ‹è¯•é…ç½®: {num_heads} ä¸ªæ³¨æ„åŠ›å¤´")
        print(f"{'='*60}\n")
        
        set_seed(BASE_CONFIG['seed'])
        
        # åˆ›å»ºæ¨¡å‹
        config = {
            **BASE_CONFIG,
            'num_heads': num_heads,
            'src_vocab_size': src_vocab_size,
            'tgt_vocab_size': tgt_vocab_size,
        }
        
        model = Transformer(
            src_vocab_size=src_vocab_size,
            tgt_vocab_size=tgt_vocab_size,
            d_model=config['d_model'],
            num_heads=num_heads,
            num_encoder_layers=config['num_encoder_layers'],
            num_decoder_layers=config['num_decoder_layers'],
            d_ff=config['d_ff'],
            dropout=config['dropout'],
            max_seq_len=config['max_seq_len'],
            pad_idx=pad_idx  # æ·»åŠ pad_idxå‚æ•°
        ).to(config['device'])
        
        # è®¡ç®—å‚æ•°é‡
        num_params = sum(p.numel() for p in model.parameters())
        print(f"æ¨¡å‹å‚æ•°é‡: {num_params:,}")
        
        # ä¼˜åŒ–å™¨ï¼ˆä½¿ç”¨ä¸train.pyä¸€è‡´çš„è®ºæ–‡é…ç½®ï¼‰
        optimizer = torch.optim.Adam(
            model.parameters(), 
            lr=config['learning_rate'],
            betas=(0.9, 0.98),
            eps=1e-9
        )
        
        # è®­ç»ƒå†å²
        train_losses = []
        val_losses = []
        train_ppls = []
        val_ppls = []
        
        # è®­ç»ƒå¾ªç¯
        print(f"\nå¼€å§‹è®­ç»ƒ ({config['num_epochs']} epochs)...")
        for epoch in range(1, config['num_epochs'] + 1):
            train_loss = train_epoch(model, train_loader, optimizer, criterion, config['device'], pad_idx)
            val_loss = evaluate(model, val_loader, criterion, config['device'], pad_idx)
            
            # è®¡ç®—å›°æƒ‘åº¦ï¼ˆä¸train.pyä¿æŒä¸€è‡´ï¼Œæ·»åŠ æº¢å‡ºä¿æŠ¤ï¼‰
            train_ppl = math.exp(train_loss) if train_loss < 10 else float('inf')
            val_ppl = math.exp(val_loss) if val_loss < 10 else float('inf')
            
            train_losses.append(train_loss)
            val_losses.append(val_loss)
            train_ppls.append(train_ppl)
            val_ppls.append(val_ppl)
            
            print(f"Epoch {epoch}/{config['num_epochs']}: "
                  f"Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, "
                  f"Val PPL={val_ppl:.2f}")
        
        # ä¿å­˜ç»“æœ
        config_name = f"{num_heads}_heads"
        all_results[config_name] = {
            'num_heads': num_heads,
            'final_train_loss': train_losses[-1],
            'final_val_loss': val_losses[-1],
            'final_train_ppl': train_ppls[-1],
            'final_val_ppl': val_ppls[-1],
            'num_params': num_params,
        }
        all_history[config_name] = {
            'train_losses': train_losses,
            'val_losses': val_losses,
            'train_ppls': train_ppls,
            'val_ppls': val_ppls,
        }
        
        print(f"âœ… {num_heads} ä¸ªå¤´è®­ç»ƒå®Œæˆ\n")
    
    # æ‰“å°ç»“æœæ‘˜è¦
    print_experiment_summary(all_results)
    
    # ä¿å­˜ç»“æœ
    results_data = {
        'config': BASE_CONFIG,
        'num_heads_tested': num_heads_list,
        'results': all_results,
        'history': all_history,
    }
    save_dir = save_results(experiment_name, results_data)
    
    # ç»˜åˆ¶å¯¹æ¯”å›¾
    print("ğŸ“Š ç”Ÿæˆå¯¹æ¯”å›¾è¡¨...")
    plot_comparison(num_heads_list, all_results, all_history, save_dir)
    
    print(f"\n{'='*60}")
    print("  å®éªŒå®Œæˆï¼")
    print(f"{'='*60}\n")


def plot_comparison(num_heads_list, results, history, save_dir):
    """ç»˜åˆ¶å¯¹æ¯”å›¾è¡¨"""
    
    colors = PLOT_CONFIG['colors']
    
    # å›¾1: è®­ç»ƒå’ŒéªŒè¯æŸå¤±æ›²çº¿
    fig, axes = plt.subplots(2, 2, figsize=(7, 6))
    plt.rcParams.update({'font.size': 10})
    
    # å­å›¾1: è®­ç»ƒæŸå¤±
    ax = axes[0, 0]
    for idx, num_heads in enumerate(num_heads_list):
        config_name = f"{num_heads}_heads"
        losses = history[config_name]['train_losses']
        ax.plot(range(1, len(losses)+1), losses, 
                label=f'{num_heads} heads', 
                color=colors[idx], linewidth=2, marker='o')
    ax.set_xlabel('Epoch', fontsize=12)
    ax.set_ylabel('è®­ç»ƒæŸå¤±', fontsize=12)
    ax.set_title('è®­ç»ƒæŸå¤±æ›²çº¿å¯¹æ¯”', fontsize=14, fontweight='bold')
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3)
    
    # å­å›¾2: éªŒè¯æŸå¤±
    ax = axes[0, 1]
    for idx, num_heads in enumerate(num_heads_list):
        config_name = f"{num_heads}_heads"
        losses = history[config_name]['val_losses']
        ax.plot(range(1, len(losses)+1), losses, 
                label=f'{num_heads} heads', 
                color=colors[idx], linewidth=2, marker='s')
    ax.set_xlabel('Epoch', fontsize=12)
    ax.set_ylabel('éªŒè¯æŸå¤±', fontsize=12)
    ax.set_title('éªŒè¯æŸå¤±æ›²çº¿å¯¹æ¯”', fontsize=14, fontweight='bold')
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3)
    
    # å­å›¾3: æœ€ç»ˆæ€§èƒ½å¯¹æ¯”ï¼ˆæŸ±çŠ¶å›¾ï¼‰
    ax = axes[1, 0]
    val_losses = [results[f"{h}_heads"]['final_val_loss'] for h in num_heads_list]
    bars = ax.bar([str(h) for h in num_heads_list], val_losses, color=colors[:len(num_heads_list)])
    ax.set_xlabel('æ³¨æ„åŠ›å¤´æ•°', fontsize=12)
    ax.set_ylabel('æœ€ç»ˆéªŒè¯æŸå¤±', fontsize=12)
    ax.set_title('æœ€ç»ˆéªŒè¯æŸå¤±å¯¹æ¯”', fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3, axis='y')
    
    # åœ¨æŸ±çŠ¶å›¾ä¸Šæ ‡æ³¨æ•°å€¼
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.4f}',
                ha='center', va='bottom', fontsize=10)
    
    # å­å›¾4: å‚æ•°é‡å¯¹æ¯”
    ax = axes[1, 1]
    params = [results[f"{h}_heads"]['num_params']/1e6 for h in num_heads_list]
    ax.plot([str(h) for h in num_heads_list], params, 
            marker='D', markersize=10, linewidth=2, color=colors[3])
    ax.set_xlabel('æ³¨æ„åŠ›å¤´æ•°', fontsize=12)
    ax.set_ylabel('å‚æ•°é‡ (M)', fontsize=12)
    ax.set_title('æ¨¡å‹å‚æ•°é‡å¯¹æ¯”', fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3)
    
    # æ ‡æ³¨æ•°å€¼
    for i, (heads, param) in enumerate(zip(num_heads_list, params)):
        ax.text(i, param, f'{param:.2f}M', 
                ha='center', va='bottom', fontsize=10)
    
    plt.tight_layout()
    save_path = save_dir / 'comparison_plots.png'
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"  âœ… å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜åˆ°: {save_path}")
    plt.close()
    
    # å›¾2: å›°æƒ‘åº¦å¯¹æ¯”
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 3.5))
    plt.rcParams.update({'font.size': 10})
    
    # éªŒè¯å›°æƒ‘åº¦æ›²çº¿
    for idx, num_heads in enumerate(num_heads_list):
        config_name = f"{num_heads}_heads"
        ppls = history[config_name]['val_ppls']
        ax1.plot(range(1, len(ppls)+1), ppls, 
                label=f'{num_heads} heads', 
                color=colors[idx], linewidth=2, marker='o')
    ax1.set_xlabel('Epoch', fontsize=12)
    ax1.set_ylabel('éªŒè¯å›°æƒ‘åº¦', fontsize=12)
    ax1.set_title('éªŒè¯å›°æƒ‘åº¦æ›²çº¿å¯¹æ¯”', fontsize=14, fontweight='bold')
    ax1.legend(fontsize=10)
    ax1.grid(True, alpha=0.3)
    
    # æœ€ç»ˆå›°æƒ‘åº¦æŸ±çŠ¶å›¾
    final_ppls = [results[f"{h}_heads"]['final_val_ppl'] for h in num_heads_list]
    bars = ax2.bar([str(h) for h in num_heads_list], final_ppls, 
                   color=colors[:len(num_heads_list)])
    ax2.set_xlabel('æ³¨æ„åŠ›å¤´æ•°', fontsize=12)
    ax2.set_ylabel('æœ€ç»ˆéªŒè¯å›°æƒ‘åº¦', fontsize=12)
    ax2.set_title('æœ€ç»ˆéªŒè¯å›°æƒ‘åº¦å¯¹æ¯”', fontsize=14, fontweight='bold')
    ax2.grid(True, alpha=0.3, axis='y')
    
    for bar in bars:
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.6f}',  # ä½¿ç”¨6ä½å°æ•°ä»¥æ˜¾ç¤ºç»†å¾®å·®å¼‚
                ha='center', va='bottom', fontsize=9)
    
    plt.tight_layout()
    save_path = save_dir / 'perplexity_comparison.png'
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"  âœ… å›°æƒ‘åº¦å¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: {save_path}")
    plt.close()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='æ¶ˆèå®éªŒ1: æ³¨æ„åŠ›å¤´æ•°')
    parser.add_argument('--gpu', type=int, default=None, help='GPU ID (0, 1, 2, 3)')
    args = parser.parse_args()
    
    run_ablation_num_heads(gpu_id=args.gpu)

