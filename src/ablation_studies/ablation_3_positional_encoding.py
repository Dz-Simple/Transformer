"""
æ¶ˆèå®éªŒ3: ä½ç½®ç¼–ç  (Positional Encoding)

æµ‹è¯•ä¸åŒä½ç½®ç¼–ç ç­–ç•¥å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼š
1. æ ‡å‡†æ­£å¼¦ä½ç½®ç¼–ç 
2. å¯å­¦ä¹ ä½ç½®ç¼–ç 
3. æ— ä½ç½®ç¼–ç 
"""
import sys
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import math  # ä½¿ç”¨math.expè€Œénp.expï¼Œä¸train.pyä¿æŒä¸€è‡´
import argparse
from pathlib import Path

sys.path.append(str(Path(__file__).parent.parent.parent))

from src.ablation_studies.ablation_config import (
    BASE_CONFIG, set_seed, save_results, prepare_data, 
    print_experiment_header, print_experiment_summary, PLOT_CONFIG
)
from src.transformer import Transformer
# from src.data.dataset import create_dataloaders
from train import train_epoch, evaluate

plt.rcParams['font.sans-serif'] = ['Noto Sans CJK JP', 'Droid Sans Fallback', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


class LearnablePositionalEncoding(nn.Module):
    """å¯å­¦ä¹ çš„ä½ç½®ç¼–ç """
    def __init__(self, d_model, max_len=5000, dropout=0.1):
        super(LearnablePositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)
        self.pe = nn.Parameter(torch.randn(1, max_len, d_model) * 0.02)
    
    def forward(self, x):
        x = x + self.pe[:, :x.size(1), :]
        return self.dropout(x)


class NoPositionalEncoding(nn.Module):
    """æ— ä½ç½®ç¼–ç ï¼ˆä»…ä¿ç•™dropoutï¼‰"""
    def __init__(self, d_model, max_len=5000, dropout=0.1):
        super(NoPositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)
    
    def forward(self, x):
        return self.dropout(x)


def create_transformer_with_pe_type(src_vocab_size, tgt_vocab_size, pad_idx, pe_type='sinusoidal'):
    """åˆ›å»ºæŒ‡å®šä½ç½®ç¼–ç ç±»å‹çš„Transformer"""
    model = Transformer(
        src_vocab_size=src_vocab_size,
        tgt_vocab_size=tgt_vocab_size,
        d_model=BASE_CONFIG['d_model'],
        num_heads=BASE_CONFIG['num_heads'],
        num_encoder_layers=BASE_CONFIG['num_encoder_layers'],
        num_decoder_layers=BASE_CONFIG['num_decoder_layers'],
        d_ff=BASE_CONFIG['d_ff'],
        dropout=BASE_CONFIG['dropout'],
        max_seq_len=BASE_CONFIG['max_seq_len'],
        pad_idx=pad_idx  # æ·»åŠ pad_idxå‚æ•°
    )
    
    # æ›¿æ¢ä½ç½®ç¼–ç æ¨¡å—
    if pe_type == 'learnable':
        model.encoder.pos_encoding = LearnablePositionalEncoding(
            BASE_CONFIG['d_model'], 
            BASE_CONFIG['max_seq_len'], 
            BASE_CONFIG['dropout']
        )
        model.decoder.pos_encoding = LearnablePositionalEncoding(
            BASE_CONFIG['d_model'], 
            BASE_CONFIG['max_seq_len'], 
            BASE_CONFIG['dropout']
        )
    elif pe_type == 'none':
        model.encoder.pos_encoding = NoPositionalEncoding(
            BASE_CONFIG['d_model'], 
            BASE_CONFIG['max_seq_len'], 
            BASE_CONFIG['dropout']
        )
        model.decoder.pos_encoding = NoPositionalEncoding(
            BASE_CONFIG['d_model'], 
            BASE_CONFIG['max_seq_len'], 
            BASE_CONFIG['dropout']
        )
    # 'sinusoidal' ä½¿ç”¨é»˜è®¤çš„ä½ç½®ç¼–ç 
    
    return model


def run_ablation_positional_encoding(gpu_id=None):
    """è¿è¡Œä½ç½®ç¼–ç æ¶ˆèå®éªŒ"""
    
    experiment_name = "ablation_3_positional_encoding"
    description = "è¯„ä¼°ä¸åŒä½ç½®ç¼–ç ç­–ç•¥(æ­£å¼¦/å¯å­¦ä¹ /æ— )å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“"
    
    # å¦‚æœæŒ‡å®šäº†GPUï¼Œè¦†ç›–é…ç½®
    if gpu_id is not None:
        BASE_CONFIG['device'] = f'cuda:{gpu_id}' if torch.cuda.is_available() else 'cpu'
        print(f"ğŸ¯ ä½¿ç”¨GPU: {BASE_CONFIG['device']}")
    
    print_experiment_header(experiment_name, description)
    
    # æµ‹è¯•çš„ä½ç½®ç¼–ç ç±»å‹
    pe_types = {
        'sinusoidal': 'æ­£å¼¦ä½ç½®ç¼–ç ',
        'learnable': 'å¯å­¦ä¹ ä½ç½®ç¼–ç ',
        'none': 'æ— ä½ç½®ç¼–ç '
    }
    
    # å‡†å¤‡æ•°æ®ï¼ˆä½¿ç”¨çœŸå®IWSLT14æ•°æ®é›†ï¼‰
    train_loader, val_loader, src_vocab_size, tgt_vocab_size, pad_idx = prepare_data(
        max_vocab_size=BASE_CONFIG['max_vocab_size'],
        batch_size=BASE_CONFIG['batch_size'],
        max_samples=BASE_CONFIG['max_samples'],
        data_dir=BASE_CONFIG['data_dir']
    )
    
    # æŸå¤±å‡½æ•°ï¼ˆä½¿ç”¨ä¸train.pyä¸€è‡´çš„LabelSmoothingï¼‰
    from train import LabelSmoothing
    criterion = LabelSmoothing(
        vocab_size=tgt_vocab_size,
        pad_idx=pad_idx,
        smoothing=0.05
    )
    
    all_results = {}
    all_history = {}
    
    for pe_type, pe_name in pe_types.items():
        print(f"\n{'='*60}")
        print(f"  æµ‹è¯•é…ç½®: {pe_name}")
        print(f"{'='*60}\n")
        
        set_seed(BASE_CONFIG['seed'])
        
        # åˆ›å»ºæ¨¡å‹
        model = create_transformer_with_pe_type(
            src_vocab_size, tgt_vocab_size, pad_idx, pe_type
        ).to(BASE_CONFIG['device'])
        
        num_params = sum(p.numel() for p in model.parameters())
        print(f"æ¨¡å‹å‚æ•°é‡: {num_params:,}")
        
        # ä¼˜åŒ–å™¨ï¼ˆä½¿ç”¨ä¸train.pyä¸€è‡´çš„è®ºæ–‡é…ç½®ï¼‰
        optimizer = torch.optim.Adam(
            model.parameters(), 
            lr=BASE_CONFIG['learning_rate'],
            betas=(0.9, 0.98),
            eps=1e-9
        )
        
        # è®­ç»ƒ
        train_losses = []
        val_losses = []
        train_ppls = []
        val_ppls = []
        
        print(f"\nå¼€å§‹è®­ç»ƒ ({BASE_CONFIG['num_epochs']} epochs)...")
        for epoch in range(1, BASE_CONFIG['num_epochs'] + 1):
            train_loss = train_epoch(model, train_loader, optimizer, criterion, BASE_CONFIG['device'], pad_idx)
            val_loss = evaluate(model, val_loader, criterion, BASE_CONFIG['device'], pad_idx)
            
            # è®¡ç®—å›°æƒ‘åº¦ï¼ˆä¸train.pyä¿æŒä¸€è‡´ï¼Œæ·»åŠ æº¢å‡ºä¿æŠ¤ï¼‰
            train_ppl = math.exp(train_loss) if train_loss < 10 else float('inf')
            val_ppl = math.exp(val_loss) if val_loss < 10 else float('inf')
            
            train_losses.append(train_loss)
            val_losses.append(val_loss)
            train_ppls.append(train_ppl)
            val_ppls.append(val_ppl)
            
            print(f"Epoch {epoch}/{BASE_CONFIG['num_epochs']}: "
                  f"Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, "
                  f"Val PPL={val_ppl:.2f}")
        
        all_results[pe_name] = {
            'pe_type': pe_type,
            'final_train_loss': train_losses[-1],
            'final_val_loss': val_losses[-1],
            'final_train_ppl': train_ppls[-1],
            'final_val_ppl': val_ppls[-1],
            'num_params': num_params,
        }
        all_history[pe_name] = {
            'train_losses': train_losses,
            'val_losses': val_losses,
            'train_ppls': train_ppls,
            'val_ppls': val_ppls,
        }
        
        print(f"âœ… {pe_name} è®­ç»ƒå®Œæˆ\n")
    
    print_experiment_summary(all_results)
    
    # ä¿å­˜ç»“æœ
    results_data = {
        'config': BASE_CONFIG,
        'pe_types_tested': pe_types,
        'results': all_results,
        'history': all_history,
    }
    save_dir = save_results(experiment_name, results_data)
    
    # ç»˜å›¾
    print("ğŸ“Š ç”Ÿæˆå¯¹æ¯”å›¾è¡¨...")
    plot_comparison(list(pe_types.values()), all_results, all_history, save_dir)
    
    print(f"\n{'='*60}")
    print("  å®éªŒå®Œæˆï¼")
    print(f"{'='*60}\n")


def plot_comparison(pe_names, results, history, save_dir):
    """ç»˜åˆ¶å¯¹æ¯”å›¾è¡¨"""
    
    colors = ['#1f77b4', '#ff7f0e', '#d62728']  # è“ã€æ©™ã€çº¢
    
    fig, axes = plt.subplots(2, 2, figsize=(7, 6))
    plt.rcParams.update({'font.size': 10})
    
    # å­å›¾1: è®­ç»ƒæŸå¤±æ›²çº¿
    ax = axes[0, 0]
    for idx, pe_name in enumerate(pe_names):
        losses = history[pe_name]['train_losses']
        ax.plot(range(1, len(losses)+1), losses, 
                label=pe_name, 
                color=colors[idx], linewidth=2.5, marker='o', markersize=6)
    ax.set_xlabel('Epoch', fontsize=12)
    ax.set_ylabel('è®­ç»ƒæŸå¤±', fontsize=12)
    ax.set_title('è®­ç»ƒæŸå¤±æ›²çº¿å¯¹æ¯”', fontsize=14, fontweight='bold')
    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.3)
    
    # å­å›¾2: éªŒè¯æŸå¤±æ›²çº¿
    ax = axes[0, 1]
    for idx, pe_name in enumerate(pe_names):
        losses = history[pe_name]['val_losses']
        ax.plot(range(1, len(losses)+1), losses, 
                label=pe_name, 
                color=colors[idx], linewidth=2.5, marker='s', markersize=6)
    ax.set_xlabel('Epoch', fontsize=12)
    ax.set_ylabel('éªŒè¯æŸå¤±', fontsize=12)
    ax.set_title('éªŒè¯æŸå¤±æ›²çº¿å¯¹æ¯”', fontsize=14, fontweight='bold')
    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.3)
    
    # å­å›¾3: æœ€ç»ˆæ€§èƒ½å¯¹æ¯”
    ax = axes[1, 0]
    val_losses = [results[name]['final_val_loss'] for name in pe_names]
    bars = ax.bar(pe_names, val_losses, color=colors)
    ax.set_ylabel('æœ€ç»ˆéªŒè¯æŸå¤±', fontsize=12)
    ax.set_title('æœ€ç»ˆéªŒè¯æŸå¤±å¯¹æ¯”', fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3, axis='y')
    plt.setp(ax.xaxis.get_majorticklabels(), rotation=15, ha='right')
    
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.4f}',
                ha='center', va='bottom', fontsize=10, fontweight='bold')
    
    # å­å›¾4: å›°æƒ‘åº¦å¯¹æ¯”
    ax = axes[1, 1]
    val_ppls = [results[name]['final_val_ppl'] for name in pe_names]
    bars = ax.bar(pe_names, val_ppls, color=colors)
    ax.set_ylabel('æœ€ç»ˆéªŒè¯å›°æƒ‘åº¦', fontsize=12)
    ax.set_title('æœ€ç»ˆéªŒè¯å›°æƒ‘åº¦å¯¹æ¯”', fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3, axis='y')
    plt.setp(ax.xaxis.get_majorticklabels(), rotation=15, ha='right')
    
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.6f}',  # ä½¿ç”¨6ä½å°æ•°ä»¥æ˜¾ç¤ºç»†å¾®å·®å¼‚
                ha='center', va='bottom', fontsize=9, fontweight='bold')
    
    plt.suptitle('ä½ç½®ç¼–ç æ¶ˆèå®éªŒç»“æœ', fontsize=16, fontweight='bold')
    plt.tight_layout()
    
    save_path = save_dir / 'position_encoding_comparison.png'
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"  âœ… å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜åˆ°: {save_path}")
    plt.close()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='æ¶ˆèå®éªŒ3: ä½ç½®ç¼–ç ')
    parser.add_argument('--gpu', type=int, default=None, help='GPU ID (0, 1, 2, 3)')
    args = parser.parse_args()
    
    run_ablation_positional_encoding(gpu_id=args.gpu)

