"""
æ¶ˆèå®éªŒ2: æ¨¡å‹å±‚æ•° (Number of Layers)

æµ‹è¯•ä¸åŒçš„ç¼–ç å™¨/è§£ç å™¨å±‚æ•°å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“
"""
import sys
import torch
import matplotlib.pyplot as plt
import math  # ä½¿ç”¨math.expè®¡ç®—å›°æƒ‘åº¦ï¼Œä¸train.pyä¿æŒä¸€è‡´
import numpy as np  # ç”¨äºisnanã€isinfã€meanç­‰è¾…åŠ©å‡½æ•°
import argparse
import time
from pathlib import Path

sys.path.append(str(Path(__file__).parent.parent.parent))

from src.ablation_studies.ablation_config import (
    BASE_CONFIG, set_seed, save_results, prepare_data, 
    print_experiment_header, print_experiment_summary, PLOT_CONFIG
)
from src.transformer import Transformer
# from src.data.dataset import create_dataloaders
from train import train_epoch, evaluate

plt.rcParams['font.sans-serif'] = ['Noto Sans CJK JP', 'Droid Sans Fallback', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


def run_ablation_num_layers(gpu_id=None):
    """è¿è¡Œå±‚æ•°æ¶ˆèå®éªŒ"""
    
    experiment_name = "ablation_2_num_layers"
    description = "è¯„ä¼°ä¸åŒæ¨¡å‹å±‚æ•°(1, 2, 3, 4, 6)å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“"
    
    # å¦‚æœæŒ‡å®šäº†GPUï¼Œè¦†ç›–é…ç½®
    if gpu_id is not None:
        BASE_CONFIG['device'] = f'cuda:{gpu_id}' if torch.cuda.is_available() else 'cpu'
        print(f"ğŸ¯ ä½¿ç”¨GPU: {BASE_CONFIG['device']}")
    
    print_experiment_header(experiment_name, description)
    
    # æµ‹è¯•çš„å±‚æ•°é…ç½®  
    num_layers_list = [1, 2, 3, 4, 6]  # å®Œæ•´æµ‹è¯•é…ç½®
    
    # å‡†å¤‡æ•°æ®ï¼ˆä½¿ç”¨çœŸå®IWSLT14æ•°æ®é›†ï¼‰
    train_loader, val_loader, src_vocab_size, tgt_vocab_size, pad_idx = prepare_data(
        max_vocab_size=BASE_CONFIG['max_vocab_size'],
        batch_size=BASE_CONFIG['batch_size'],
        max_samples=BASE_CONFIG['max_samples'],
        data_dir=BASE_CONFIG['data_dir']
    )
    
    # æŸå¤±å‡½æ•°ï¼ˆä½¿ç”¨ä¸train.pyä¸€è‡´çš„LabelSmoothingï¼‰
    from train import LabelSmoothing
    criterion = LabelSmoothing(
        vocab_size=tgt_vocab_size,
        pad_idx=pad_idx,
        smoothing=0.05
    )
    
    all_results = {}
    all_history = {}
    
    for num_layers in num_layers_list:
        print(f"\n{'='*60}")
        print(f"  æµ‹è¯•é…ç½®: {num_layers} å±‚ç¼–ç å™¨/è§£ç å™¨")
        print(f"{'='*60}\n")
        
        set_seed(BASE_CONFIG['seed'])
        
        # åˆ›å»ºæ¨¡å‹
        model = Transformer(
            src_vocab_size=src_vocab_size,
            tgt_vocab_size=tgt_vocab_size,
            d_model=BASE_CONFIG['d_model'],
            num_heads=BASE_CONFIG['num_heads'],
            num_encoder_layers=num_layers,
            num_decoder_layers=num_layers,
            d_ff=BASE_CONFIG['d_ff'],
            dropout=BASE_CONFIG['dropout'],
            max_seq_len=BASE_CONFIG['max_seq_len'],
            pad_idx=pad_idx  # æ·»åŠ pad_idxå‚æ•°
        ).to(BASE_CONFIG['device'])
        
        num_params = sum(p.numel() for p in model.parameters())
        print(f"æ¨¡å‹å‚æ•°é‡: {num_params:,}")
        
        # ä¼˜åŒ–å™¨ï¼ˆä½¿ç”¨ä¸train.pyä¸€è‡´çš„è®ºæ–‡é…ç½®ï¼‰
        optimizer = torch.optim.Adam(
            model.parameters(), 
            lr=BASE_CONFIG['learning_rate'],
            betas=(0.9, 0.98),
            eps=1e-9
        )
        
        # è®­ç»ƒ
        train_losses = []
        val_losses = []
        train_ppls = []
        val_ppls = []
        epoch_times = []
        
        print(f"\nå¼€å§‹è®­ç»ƒ ({BASE_CONFIG['num_epochs']} epochs)...")
        for epoch in range(1, BASE_CONFIG['num_epochs'] + 1):
            start_time = time.time()
            train_loss = train_epoch(model, train_loader, optimizer, criterion, BASE_CONFIG['device'], pad_idx)
            val_loss = evaluate(model, val_loader, criterion, BASE_CONFIG['device'], pad_idx)
            epoch_time = time.time() - start_time
            
            # è®¡ç®—å›°æƒ‘åº¦ï¼ˆä¸train.pyä¿æŒä¸€è‡´ï¼Œæ·»åŠ æº¢å‡ºä¿æŠ¤ï¼‰
            train_ppl = math.exp(train_loss) if train_loss < 10 else float('inf')
            val_ppl = math.exp(val_loss) if val_loss < 10 else float('inf')
            
            train_losses.append(train_loss)
            val_losses.append(val_loss)
            train_ppls.append(train_ppl)
            val_ppls.append(val_ppl)
            epoch_times.append(epoch_time)
            
            print(f"Epoch {epoch}/{BASE_CONFIG['num_epochs']}: "
                  f"Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, "
                  f"Time={epoch_time:.1f}s")
        
        config_name = f"{num_layers}_layers"
        all_results[config_name] = {
            'num_layers': num_layers,
            'final_train_loss': train_losses[-1],
            'final_val_loss': val_losses[-1],
            'final_train_ppl': train_ppls[-1],
            'final_val_ppl': val_ppls[-1],
            'num_params': num_params,
            'avg_epoch_time': np.mean(epoch_times),
        }
        all_history[config_name] = {
            'train_losses': train_losses,
            'val_losses': val_losses,
            'train_ppls': train_ppls,
            'val_ppls': val_ppls,
            'epoch_times': epoch_times,
        }
        
        print(f"âœ… {num_layers} å±‚è®­ç»ƒå®Œæˆ\n")
    
    print_experiment_summary(all_results)
    
    # ä¿å­˜ç»“æœ
    results_data = {
        'config': BASE_CONFIG,
        'num_layers_tested': num_layers_list,
        'results': all_results,
        'history': all_history,
    }
    save_dir = save_results(experiment_name, results_data)
    
    # ç»˜å›¾
    print("ğŸ“Š ç”Ÿæˆå¯¹æ¯”å›¾è¡¨...")
    plot_comparison(num_layers_list, all_results, all_history, save_dir)
    
    print(f"\n{'='*60}")
    print("  å®éªŒå®Œæˆï¼")
    print(f"{'='*60}\n")


def plot_comparison(num_layers_list, results, history, save_dir):
    """ç»˜åˆ¶å¯¹æ¯”å›¾è¡¨"""
    
    colors = PLOT_CONFIG['colors']
    
    # å›¾1: æ€§èƒ½ã€å‚æ•°é‡å’Œæ—¶é—´çš„ç»¼åˆå¯¹æ¯”
    fig = plt.figure(figsize=(7, 10))
    plt.rcParams.update({'font.size': 10})
    
    # å­å›¾1: éªŒè¯æŸå¤±æ›²çº¿
    ax1 = plt.subplot(3, 2, 1)
    for idx, num_layers in enumerate(num_layers_list):
        config_name = f"{num_layers}_layers"
        losses = history[config_name]['val_losses']
        ax1.plot(range(1, len(losses)+1), losses, 
                label=f'{num_layers} å±‚', 
                color=colors[idx], linewidth=2, marker='o')
    ax1.set_xlabel('Epoch', fontsize=11)
    ax1.set_ylabel('éªŒè¯æŸå¤±', fontsize=11)
    ax1.set_title('éªŒè¯æŸå¤±æ›²çº¿', fontsize=13, fontweight='bold')
    ax1.legend(fontsize=9)
    ax1.grid(True, alpha=0.3)
    
    # å­å›¾2: å›°æƒ‘åº¦æ›²çº¿
    ax2 = plt.subplot(3, 2, 2)
    for idx, num_layers in enumerate(num_layers_list):
        config_name = f"{num_layers}_layers"
        ppls = history[config_name]['val_ppls']
        ax2.plot(range(1, len(ppls)+1), ppls, 
                label=f'{num_layers} å±‚', 
                color=colors[idx], linewidth=2, marker='s')
    ax2.set_xlabel('Epoch', fontsize=11)
    ax2.set_ylabel('éªŒè¯å›°æƒ‘åº¦', fontsize=11)
    ax2.set_title('éªŒè¯å›°æƒ‘åº¦æ›²çº¿', fontsize=13, fontweight='bold')
    ax2.legend(fontsize=9)
    ax2.grid(True, alpha=0.3)
    
    # å­å›¾3: æœ€ç»ˆæ€§èƒ½å¯¹æ¯”
    ax3 = plt.subplot(3, 2, 3)
    val_losses = [results[f"{l}_layers"]['final_val_loss'] for l in num_layers_list]
    bars = ax3.bar([str(l) for l in num_layers_list], val_losses, 
                   color=colors[:len(num_layers_list)])
    ax3.set_xlabel('å±‚æ•°', fontsize=11)
    ax3.set_ylabel('æœ€ç»ˆéªŒè¯æŸå¤±', fontsize=11)
    ax3.set_title('æœ€ç»ˆæ€§èƒ½å¯¹æ¯”', fontsize=13, fontweight='bold')
    ax3.grid(True, alpha=0.3, axis='y')
    for bar in bars:
        height = bar.get_height()
        ax3.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.3f}', ha='center', va='bottom', fontsize=9)
    
    # å­å›¾4: å‚æ•°é‡å¯¹æ¯”
    ax4 = plt.subplot(3, 2, 4)
    params = [results[f"{l}_layers"]['num_params']/1e6 for l in num_layers_list]
    ax4.plot([str(l) for l in num_layers_list], params, 
            marker='D', markersize=10, linewidth=2, color=colors[2])
    ax4.set_xlabel('å±‚æ•°', fontsize=11)
    ax4.set_ylabel('å‚æ•°é‡ (M)', fontsize=11)
    ax4.set_title('æ¨¡å‹å‚æ•°é‡', fontsize=13, fontweight='bold')
    ax4.grid(True, alpha=0.3)
    for i, (layers, param) in enumerate(zip(num_layers_list, params)):
        ax4.text(i, param, f'{param:.1f}M', 
                ha='center', va='bottom', fontsize=9)
    
    # å­å›¾5: è®­ç»ƒæ—¶é—´å¯¹æ¯”
    ax5 = plt.subplot(3, 2, 5)
    times = [results[f"{l}_layers"]['avg_epoch_time'] for l in num_layers_list]
    bars = ax5.bar([str(l) for l in num_layers_list], times, 
                   color=colors[:len(num_layers_list)])
    ax5.set_xlabel('å±‚æ•°', fontsize=11)
    ax5.set_ylabel('å¹³å‡Epochæ—¶é—´ (ç§’)', fontsize=11)
    ax5.set_title('è®­ç»ƒæ—¶é—´å¯¹æ¯”', fontsize=13, fontweight='bold')
    ax5.grid(True, alpha=0.3, axis='y')
    for bar in bars:
        height = bar.get_height()
        ax5.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.1f}s', ha='center', va='bottom', fontsize=9)
    
    # å­å›¾6: æ€§èƒ½-æ•ˆç‡æƒè¡¡
    ax6 = plt.subplot(3, 2, 6)
    # ç»˜åˆ¶æ€§èƒ½vså‚æ•°é‡
    ax6_twin = ax6.twinx()
    line1 = ax6.plot([str(l) for l in num_layers_list], val_losses, 
                     marker='o', markersize=10, linewidth=2, 
                     color=colors[0], label='éªŒè¯æŸå¤±')
    line2 = ax6_twin.plot([str(l) for l in num_layers_list], params, 
                          marker='s', markersize=10, linewidth=2, 
                          color=colors[1], label='å‚æ•°é‡(M)')
    ax6.set_xlabel('å±‚æ•°', fontsize=11)
    ax6.set_ylabel('éªŒè¯æŸå¤±', fontsize=11, color=colors[0])
    ax6_twin.set_ylabel('å‚æ•°é‡ (M)', fontsize=11, color=colors[1])
    ax6.set_title('æ€§èƒ½-å‚æ•°é‡æƒè¡¡', fontsize=13, fontweight='bold')
    ax6.tick_params(axis='y', labelcolor=colors[0])
    ax6_twin.tick_params(axis='y', labelcolor=colors[1])
    ax6.grid(True, alpha=0.3)
    
    # åˆå¹¶å›¾ä¾‹
    lines = line1 + line2
    labels = [l.get_label() for l in lines]
    ax6.legend(lines, labels, loc='upper left', fontsize=9)
    
    plt.suptitle('æ¨¡å‹å±‚æ•°æ¶ˆèå®éªŒç»“æœ', fontsize=16, fontweight='bold', y=0.995)
    plt.tight_layout()
    
    save_path = save_dir / 'comprehensive_comparison.png'
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"  âœ… ç»¼åˆå¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: {save_path}")
    plt.close()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='æ¶ˆèå®éªŒ2: æ¨¡å‹å±‚æ•°')
    parser.add_argument('--gpu', type=int, default=None, help='GPU ID (0, 1, 2, 3)')
    args = parser.parse_args()
    
    run_ablation_num_layers(gpu_id=args.gpu)

