"""
Transformer æœºå™¨ç¿»è¯‘æµ‹è¯•è„šæœ¬

åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè®¡ç®—BLEUç­‰è¯„ä»·æŒ‡æ ‡å¹¶ç»˜åˆ¶å›¾åƒ
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import os
import argparse
from tqdm import tqdm
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

from src import Transformer, generate_causal_mask
from src.data import (
    SimpleTokenizer,
    TranslationDataset,
    collate_fn,
    load_vocabulary_from_file
)

# é…ç½®ä¸­æ–‡å­—ä½“ - ä½¿ç”¨ç³»ç»Ÿä¸­æ”¯æŒä¸­æ–‡çš„å­—ä½“
plt.rcParams['font.sans-serif'] = ['Noto Sans CJK JP', 'Droid Sans Fallback', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜

try:
    from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
    from nltk.translate.meteor_score import meteor_score
    import nltk
    NLTK_AVAILABLE = True
    
    # ä¼˜åŒ–ï¼šåªåœ¨ç¬¬ä¸€æ¬¡å¤±è´¥æ—¶æ‰æ£€æŸ¥å’Œä¸‹è½½
    # å¦‚æœimportæˆåŠŸï¼Œè¯´æ˜NLTKæ•°æ®å¤§æ¦‚ç‡å¯ç”¨
    # é¿å…æ¯æ¬¡éƒ½è°ƒç”¨ nltk.data.find() å¯¼è‡´å¡é¡¿
    
except ImportError:
    NLTK_AVAILABLE = False
    print("è­¦å‘Š: NLTKæœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€åŒ–çš„BLEUè®¡ç®—")
except Exception as e:
    NLTK_AVAILABLE = False
    print(f"è­¦å‘Š: NLTKåˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨ç®€åŒ–çš„BLEUè®¡ç®—")


def create_masks(src, tgt, pad_idx):
    """
    åˆ›å»ºæºåºåˆ—å’Œç›®æ ‡åºåˆ—çš„æ©ç 
    Args:
        src: æºåºåˆ— [batch, src_len]
        tgt: ç›®æ ‡åºåˆ— [batch, tgt_len]
        pad_idx: paddingç´¢å¼•
    Returns:
        src_mask, tgt_mask
    """
    batch_size = src.size(0)
    src_len = src.size(1)
    tgt_len = tgt.size(1)
    
    # æºåºåˆ—æ©ç ï¼šmaskæ‰paddingä½ç½®
    src_mask = (src != pad_idx).unsqueeze(1).unsqueeze(2)
    
    # ç›®æ ‡åºåˆ—æ©ç ï¼šå› æœæ©ç  + paddingæ©ç 
    tgt_causal_mask = generate_causal_mask(tgt_len).bool().to(src.device)
    tgt_pad_mask = (tgt != pad_idx).unsqueeze(1)
    tgt_mask = tgt_causal_mask.unsqueeze(0) & tgt_pad_mask.unsqueeze(2)
    
    return src_mask, tgt_mask


def load_test_data(data_dir="datasets/iwslt14"):
    """
    åŠ è½½æµ‹è¯•é›†æ•°æ®
    """
    test_src_file = os.path.join(data_dir, "test.de")
    test_tgt_file = os.path.join(data_dir, "test.en")
    
    if not os.path.exists(test_src_file) or not os.path.exists(test_tgt_file):
        raise FileNotFoundError(f"æµ‹è¯•æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {test_src_file} æˆ– {test_tgt_file}")
    
    print("æ­£åœ¨åŠ è½½æµ‹è¯•é›†...")
    test_src = []
    test_tgt = []
    with open(test_src_file, 'r', encoding='utf-8') as f:
        test_src = [line.strip() for line in f if line.strip()]
    with open(test_tgt_file, 'r', encoding='utf-8') as f:
        test_tgt = [line.strip() for line in f if line.strip()]
    
    if len(test_src) != len(test_tgt):
        raise ValueError(f"æµ‹è¯•é›†æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€æ•°é‡ä¸åŒ¹é…: {len(test_src)} vs {len(test_tgt)}")
    
    print(f"âœ… æµ‹è¯•é›†åŠ è½½æˆåŠŸ: {len(test_src)} æ¡")
    return test_src, test_tgt


def calculate_bleu(reference, candidate, smoothing=None):
    """
    è®¡ç®—BLEUåˆ†æ•°
    """
    if not NLTK_AVAILABLE:
        # ç®€åŒ–çš„BLEUè®¡ç®—ï¼ˆåŸºäºn-gramé‡å ï¼‰
        ref_tokens = reference.lower().split()
        cand_tokens = candidate.lower().split()
        
        if len(ref_tokens) == 0 or len(cand_tokens) == 0:
            return 0.0
        
        # ç®€å•çš„1-gramç²¾ç¡®åº¦
        ref_set = set(ref_tokens)
        cand_set = set(cand_tokens)
        if len(cand_set) == 0:
            return 0.0
        precision = len(ref_set & cand_set) / len(cand_set)
        
        # ç®€å•çš„å¬å›ç‡
        recall = len(ref_set & cand_set) / len(ref_set) if len(ref_set) > 0 else 0.0
        
        # ç®€å•çš„F1
        if precision + recall == 0:
            return 0.0
        return 2 * precision * recall / (precision + recall)
    
    try:
        ref_tokens = reference.split()
        cand_tokens = candidate.split()
        
        if smoothing is None:
            smoothing = SmoothingFunction().method1
        
        # sentence_bleuéœ€è¦referenceæ˜¯åˆ—è¡¨çš„åˆ—è¡¨
        return sentence_bleu([ref_tokens], cand_tokens, smoothing_function=smoothing)
    except Exception as e:
        # å¦‚æœNLTKæ•°æ®ç¼ºå¤±ï¼Œé™çº§åˆ°ç®€åŒ–è®¡ç®—
        ref_tokens = reference.lower().split()
        cand_tokens = candidate.lower().split()
        if len(ref_tokens) == 0 or len(cand_tokens) == 0:
            return 0.0
        ref_set = set(ref_tokens)
        cand_set = set(cand_tokens)
        if len(cand_set) == 0:
            return 0.0
        precision = len(ref_set & cand_set) / len(cand_set)
        recall = len(ref_set & cand_set) / len(ref_set) if len(ref_set) > 0 else 0.0
        if precision + recall == 0:
            return 0.0
        return 2 * precision * recall / (precision + recall)


def calculate_meteor(reference, candidate):
    """
    è®¡ç®—METEORåˆ†æ•°
    """
    if not NLTK_AVAILABLE:
        return 0.0
    
    try:
        ref_tokens = reference.split()
        cand_tokens = candidate.split()
        return meteor_score([ref_tokens], cand_tokens)
    except Exception as e:
        # METEORå¯èƒ½éœ€è¦é¢å¤–çš„NLTKæ•°æ®ï¼ˆwordnetï¼‰ï¼Œå¤±è´¥æ—¶è¿”å›0
        return 0.0


def translate_batch(model, src_batch, src_vocab, tgt_vocab, tokenizer, device, max_len=80):
    """
    æ‰¹é‡ç¿»è¯‘å¥å­
    """
    model.eval()
    translations = []
    
    with torch.no_grad():
        for src_text in src_batch:
            # åˆ†è¯å’Œç¼–ç 
            src_tokens = tokenizer.tokenize(src_text)
            src_indices = src_vocab.encode(src_tokens, add_special_tokens=True)
            src = torch.tensor([src_indices]).to(device)
            
            # ç”Ÿæˆ
            generated = model.generate(
                src,
                max_len=max_len,
                start_token=tgt_vocab.sos_idx,
                end_token=tgt_vocab.eos_idx
            )
            
            # è§£ç 
            tgt_indices = generated[0].tolist()
            tgt_tokens = tgt_vocab.decode(tgt_indices, skip_special_tokens=True)
            translation = tokenizer.detokenize(tgt_tokens)
            translations.append(translation)
    
    return translations


def evaluate_on_test_set(model, test_src, test_tgt, src_vocab, tgt_vocab, tokenizer, device, batch_size=16, max_samples=None):
    """
    åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹
    """
    print("\nå¼€å§‹æµ‹è¯•é›†è¯„ä¼°...")
    
    # é™åˆ¶æµ‹è¯•æ ·æœ¬æ•°é‡ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
    if max_samples is not None:
        test_src = test_src[:max_samples]
        test_tgt = test_tgt[:max_samples]
        print(f"é™åˆ¶æµ‹è¯•æ ·æœ¬æ•°: {len(test_src)}")
    
    model.eval()
    all_translations = []
    all_references = []
    
    # æ‰¹é‡ç¿»è¯‘
    print("æ­£åœ¨ç”Ÿæˆç¿»è¯‘...")
    for i in tqdm(range(0, len(test_src), batch_size), desc="ç¿»è¯‘è¿›åº¦"):
        batch_src = test_src[i:i+batch_size]
        batch_translations = translate_batch(model, batch_src, src_vocab, tgt_vocab, tokenizer, device)
        all_translations.extend(batch_translations)
        all_references.extend(test_tgt[i:i+batch_size])
    
    # å¯¹å‚è€ƒç¿»è¯‘ä¹Ÿè¿›è¡Œ detokenizeï¼ˆç§»é™¤ BPE æ ‡è®°ï¼‰
    print("\nå¤„ç†å‚è€ƒç¿»è¯‘ï¼ˆç§»é™¤BPEæ ‡è®°ï¼‰...")
    all_references_detok = []
    for ref in all_references:
        # å¯¹å‚è€ƒç¿»è¯‘è¿›è¡Œç®€å•çš„åˆ†è¯ç„¶ådetokenizeï¼Œä»¥ç§»é™¤@@æ ‡è®°
        ref_tokens = ref.split()  # ç®€å•ç©ºæ ¼åˆ†è¯
        ref_detok = tokenizer.detokenize(ref_tokens)
        all_references_detok.append(ref_detok)
    
    # è®¡ç®—è¯„ä»·æŒ‡æ ‡
    print("\næ­£åœ¨è®¡ç®—è¯„ä»·æŒ‡æ ‡...")
    bleu_scores = []
    meteor_scores = []
    
    smoothing = SmoothingFunction().method1 if NLTK_AVAILABLE else None
    
    for ref, cand in tqdm(zip(all_references_detok, all_translations), desc="è®¡ç®—æŒ‡æ ‡", total=len(all_references_detok)):
        bleu = calculate_bleu(ref, cand, smoothing)
        bleu_scores.append(bleu)
        
        if NLTK_AVAILABLE:
            meteor = calculate_meteor(ref, cand)
            meteor_scores.append(meteor)
    
    # è®¡ç®—å¹³å‡å€¼
    avg_bleu = np.mean(bleu_scores)
    avg_meteor = np.mean(meteor_scores) if meteor_scores else 0.0
    
    return {
        'translations': all_translations,
        'references': all_references_detok,  # è¿”å›å¤„ç†åçš„å‚è€ƒç¿»è¯‘
        'bleu_scores': bleu_scores,
        'meteor_scores': meteor_scores,
        'avg_bleu': avg_bleu,
        'avg_meteor': avg_meteor
    }


def plot_evaluation_results(results, save_dir):
    """
    ç»˜åˆ¶è¯„ä¼°ç»“æœå›¾åƒ
    å‚è€ƒtrain.pyçš„ç»˜å›¾é£æ ¼ï¼Œå›¾ç‰‡å¤§å°ä¿æŒä¸€è‡´
    """
    fig, axes = plt.subplots(1, 2, figsize=(7, 3.5))
    plt.rcParams.update({'font.size': 10})
    
    # BLEUåˆ†æ•°åˆ†å¸ƒ
    ax1 = axes[0]
    bleu_scores = results['bleu_scores']
    ax1.hist(bleu_scores, bins=20, edgecolor='black', alpha=0.7, color='skyblue')
    ax1.axvline(results['avg_bleu'], color='red', linestyle='--', linewidth=2, label=f'å¹³å‡BLEU: {results["avg_bleu"]:.4f}')
    ax1.set_xlabel('BLEUåˆ†æ•°')
    ax1.set_ylabel('æ ·æœ¬æ•°é‡')
    ax1.set_title('BLEUåˆ†æ•°åˆ†å¸ƒ')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # METEORåˆ†æ•°åˆ†å¸ƒï¼ˆå¦‚æœå¯ç”¨ï¼‰
    ax2 = axes[1]
    if results['meteor_scores'] and len(results['meteor_scores']) > 0:
        meteor_scores = results['meteor_scores']
        ax2.hist(meteor_scores, bins=20, edgecolor='black', alpha=0.7, color='lightcoral')
        ax2.axvline(results['avg_meteor'], color='blue', linestyle='--', linewidth=2, label=f'å¹³å‡METEOR: {results["avg_meteor"]:.4f}')
        ax2.set_xlabel('METEORåˆ†æ•°')
        ax2.set_ylabel('æ ·æœ¬æ•°é‡')
        ax2.set_title('METEORåˆ†æ•°åˆ†å¸ƒ')
        ax2.legend()
    else:
        # å¦‚æœæ²¡æœ‰METEORåˆ†æ•°ï¼Œæ˜¾ç¤ºBLEUåˆ†æ•°çš„ç®±çº¿å›¾
        ax2.boxplot(bleu_scores, vert=True, patch_artist=True,
                   boxprops=dict(facecolor='lightblue', alpha=0.7))
        ax2.set_ylabel('BLEUåˆ†æ•°')
        ax2.set_title('BLEUåˆ†æ•°ç®±çº¿å›¾')
        ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    save_path = os.path.join(save_dir, 'test_evaluation_results.png')
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"  è¯„ä¼°ç»“æœå›¾è¡¨å·²ä¿å­˜åˆ°: {save_path}")


def save_evaluation_log(results, save_dir):
    """
    ä¿å­˜è¯„ä¼°æ—¥å¿—
    """
    # åˆ›å»ºè¯¦ç»†çš„æ•°æ®æ¡†
    data = {
        'å‚è€ƒç¿»è¯‘': results['references'],
        'æ¨¡å‹ç¿»è¯‘': results['translations'],
        'BLEUåˆ†æ•°': [f'{score:.4f}' for score in results['bleu_scores']]
    }
    
    if results['meteor_scores']:
        data['METEORåˆ†æ•°'] = [f'{score:.4f}' for score in results['meteor_scores']]
    
    df = pd.DataFrame(data)
    
    # ä¿å­˜ä¸ºCSV
    csv_path = os.path.join(save_dir, 'test_evaluation_log.csv')
    df.to_csv(csv_path, index=False, encoding='utf-8')
    print(f"  è¯¦ç»†è¯„ä¼°æ—¥å¿—å·²ä¿å­˜åˆ°: {csv_path}")
    
    # ä¿å­˜æ±‡æ€»æŠ¥å‘Š
    summary = {
        'è¯„ä»·æŒ‡æ ‡': ['å¹³å‡BLEUåˆ†æ•°', 'å¹³å‡METEORåˆ†æ•°', 'æµ‹è¯•æ ·æœ¬æ•°'],
        'æ•°å€¼': [
            f'{results["avg_bleu"]:.4f}',
            f'{results["avg_meteor"]:.4f}' if results['meteor_scores'] else 'N/A',
            len(results['references'])
        ]
    }
    
    summary_df = pd.DataFrame(summary)
    
    # ä¿å­˜æ±‡æ€»ä¸ºCSV
    summary_csv_path = os.path.join(save_dir, 'test_evaluation_summary.csv')
    summary_df.to_csv(summary_csv_path, index=False, encoding='utf-8')
    
    # ä¿å­˜ä¸ºMarkdown
    md_path = os.path.join(save_dir, 'test_evaluation_summary.md')
    with open(md_path, 'w', encoding='utf-8') as f:
        f.write("# æµ‹è¯•é›†è¯„ä¼°ç»“æœæ±‡æ€»\n\n")
        f.write(f"## è¯„ä»·æŒ‡æ ‡\n\n")
        f.write(summary_df.to_markdown(index=False))
        f.write("\n\n")
        f.write("## è¯¦ç»†ç»“æœ\n\n")
        f.write("è¯¦ç»†ç»“æœè¯·æŸ¥çœ‹ test_evaluation_log.csv æ–‡ä»¶ã€‚\n")
    
    print(f"  è¯„ä¼°æ±‡æ€»å·²ä¿å­˜åˆ°: {md_path}")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æµ‹è¯•Transformeræœºå™¨ç¿»è¯‘æ¨¡å‹')
    parser.add_argument('--model_path', type=str, default='results/models_20251103_204641/best_model.pt',
                       help='è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„ (ä¾‹å¦‚: results/models_20251102_153059/best_model.pt)')
    parser.add_argument('--data_dir', type=str, default='datasets/iwslt14',
                       help='æ•°æ®é›†ç›®å½•è·¯å¾„')
    parser.add_argument('--batch_size', type=int, default=16,
                       help='ç¿»è¯‘æ—¶çš„æ‰¹é‡å¤§å°ï¼ˆæ³¨æ„ï¼šå½“å‰å®ç°æ˜¯é€å¥ç¿»è¯‘ï¼Œbatch_sizeä¸»è¦å½±å“å†…å­˜å¾ªç¯æ¬¡æ•°ï¼‰')
    parser.add_argument('--max_samples', type=int, default=None,
                       help='é™åˆ¶æµ‹è¯•æ ·æœ¬æ•°é‡ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨å…¨éƒ¨ï¼‰')
    parser.add_argument('--device', type=str, default=None,
                       help='è®¾å¤‡ (cuda:0, cpuç­‰)ï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨é€‰æ‹©')
    
    args = parser.parse_args()
    
    print("="*60)
    print("Transformer æœºå™¨ç¿»è¯‘æ¨¡å‹æµ‹è¯•")
    print("="*60)
    
    # è®¾ç½®è®¾å¤‡
    if args.device is None:
        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    else:
        device = torch.device(args.device)
    
    print(f"\nä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ¨¡å‹
    print(f"\næ­£åœ¨åŠ è½½æ¨¡å‹: {args.model_path}")
    if not os.path.exists(args.model_path):
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.model_path}")
    
    checkpoint = torch.load(args.model_path, map_location=device)
    config = checkpoint['config']
    src_vocab = checkpoint['src_vocab']
    tgt_vocab = checkpoint['tgt_vocab']
    
    print(f"æ¨¡å‹é…ç½®:")
    print(f"  d_model: {config['d_model']}")
    print(f"  num_heads: {config['num_heads']}")
    print(f"  num_encoder_layers: {config['num_encoder_layers']}")
    print(f"  num_decoder_layers: {config['num_decoder_layers']}")
    print(f"  batch_size: {config['batch_size']}")
    
    # åˆ›å»ºæ¨¡å‹
    model = Transformer(
        src_vocab_size=len(src_vocab),
        tgt_vocab_size=len(tgt_vocab),
        d_model=config['d_model'],
        num_heads=config['num_heads'],
        num_encoder_layers=config['num_encoder_layers'],
        num_decoder_layers=config['num_decoder_layers'],
        d_ff=config['d_ff'],
        dropout=config['dropout'],
        pad_idx=src_vocab.pad_idx  # ğŸ”§ ä¿®å¤: ä¼ é€’paddingç´¢å¼•
    ).to(device)
    
    # åŠ è½½æ¨¡å‹æƒé‡
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    # åˆ›å»ºåˆ†è¯å™¨
    tokenizer = SimpleTokenizer(lowercase=False)
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    test_src, test_tgt = load_test_data(args.data_dir)
    
    # è¯„ä¼°æ¨¡å‹
    results = evaluate_on_test_set(
        model, test_src, test_tgt, src_vocab, tgt_vocab, 
        tokenizer, device, batch_size=args.batch_size,
        max_samples=args.max_samples
    )
    
    # æ‰“å°ç»“æœ
    print("\n" + "="*60)
    print("æµ‹è¯•é›†è¯„ä¼°ç»“æœ")
    print("="*60)
    print(f"\nå¹³å‡BLEUåˆ†æ•°: {results['avg_bleu']:.4f}")
    if results['meteor_scores']:
        print(f"å¹³å‡METEORåˆ†æ•°: {results['avg_meteor']:.4f}")
    print(f"æµ‹è¯•æ ·æœ¬æ•°: {len(results['references'])}")
    
    # ä¿å­˜ç»“æœ
    model_dir = os.path.dirname(args.model_path)
    test_results_dir = os.path.join(model_dir, 'test_results')
    os.makedirs(test_results_dir, exist_ok=True)
    
    print("\nä¿å­˜è¯„ä¼°ç»“æœ...")
    plot_evaluation_results(results, test_results_dir)
    save_evaluation_log(results, test_results_dir)
    
    print("\n" + "="*60)
    print("æµ‹è¯•å®Œæˆï¼")
    print("="*60)
    print(f"\nç»“æœä¿å­˜ä½ç½®: {test_results_dir}")


if __name__ == "__main__":
    main()

