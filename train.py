"""
Transformer æœºå™¨ç¿»è¯‘è®­ç»ƒè„šæœ¬

ä½¿ç”¨IWSLT 2017æ•°æ®é›†è®­ç»ƒTransformeræ¨¡å‹
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import time
import math
import os
from datetime import datetime
from tqdm import tqdm
import matplotlib.pyplot as plt
import pandas as pd

from src import Transformer, generate_causal_mask
from src.data import (
    SimpleTokenizer,
    TranslationDataset,
    collate_fn,
    load_iwslt14_de_en,
    load_vocabulary_from_file
)

# é…ç½®ä¸­æ–‡å­—ä½“ - ä½¿ç”¨ç³»ç»Ÿä¸­æ”¯æŒä¸­æ–‡çš„å­—ä½“
plt.rcParams['font.sans-serif'] = ['Noto Sans CJK JP', 'Droid Sans Fallback', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜


class NoamScheduler:
    """
    Noamå­¦ä¹ ç‡è°ƒåº¦å™¨ (æ¥è‡ª "Attention Is All You Need" è®ºæ–‡)
    
    å…¬å¼: lrate = d_model^(-0.5) * min(step^(-0.5), step * warmup_steps^(-1.5))
    """
    
    def __init__(self, optimizer, d_model, warmup_steps=4000, factor=1.0):
        """
        Args:
            optimizer: ä¼˜åŒ–å™¨
            d_model: æ¨¡å‹ç»´åº¦
            warmup_steps: warmupæ­¥æ•°ï¼ˆé»˜è®¤4000ï¼Œæ ¹æ®è®ºæ–‡ï¼‰
            factor: ç¼©æ”¾å› å­ï¼ˆé»˜è®¤1.0ï¼‰
        """
        self.optimizer = optimizer
        self.d_model = d_model
        self.warmup_steps = warmup_steps
        self.factor = factor
        self._step = 0
        # åˆå§‹åŒ–æ—¶è®¡ç®—åˆå§‹å­¦ä¹ ç‡ï¼ˆstep=1æ—¶çš„å­¦ä¹ ç‡ï¼‰
        self._rate = self._calculate_rate(1)
    
    def _calculate_rate(self, step):
        """è®¡ç®—æŒ‡å®šæ­¥æ•°çš„å­¦ä¹ ç‡"""
        return self.factor * (
            self.d_model ** (-0.5) *
            min(step ** (-0.5), step * self.warmup_steps ** (-1.5))
        )
    
    def step(self):
        """æ›´æ–°å­¦ä¹ ç‡"""
        self._step += 1
        rate = self._rate = self._calculate_rate(self._step)
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = rate
        return rate
    
    def get_lr(self):
        """è·å–å½“å‰å­¦ä¹ ç‡"""
        if self._step == 0:
            # å¦‚æœè¿˜æ²¡æœ‰æ‰§è¡Œè¿‡stepï¼Œè¿”å›step=1æ—¶çš„å­¦ä¹ ç‡
            return self._calculate_rate(1)
        return self._rate
    
    def state_dict(self):
        """ä¿å­˜çŠ¶æ€"""
        return {
            'step': self._step,
            'warmup_steps': self.warmup_steps,
            'factor': self.factor
        }
    
    def load_state_dict(self, state_dict):
        """åŠ è½½çŠ¶æ€"""
        self._step = state_dict['step']
        self.warmup_steps = state_dict.get('warmup_steps', 4000)
        self.factor = state_dict.get('factor', 1.0)


def create_masks(src, tgt, pad_idx):
    """
    åˆ›å»ºæºåºåˆ—å’Œç›®æ ‡åºåˆ—çš„æ©ç 
    Args:
        src: æºåºåˆ— [batch, src_len]
        tgt: ç›®æ ‡åºåˆ— [batch, tgt_len]
        pad_idx: paddingç´¢å¼•
    Returns:
        src_mask, tgt_mask
    """
    batch_size = src.size(0)
    src_len = src.size(1)
    tgt_len = tgt.size(1)
    
    # æºåºåˆ—æ©ç ï¼šmaskæ‰paddingä½ç½®
    # [batch, 1, 1, src_len]
    src_mask = (src != pad_idx).unsqueeze(1).unsqueeze(2)
    
    # ç›®æ ‡åºåˆ—æ©ç ï¼šå› æœæ©ç  + paddingæ©ç 
    # å› æœæ©ç : [1, tgt_len, tgt_len]
    tgt_causal_mask = generate_causal_mask(tgt_len).bool().to(src.device)
    
    # Paddingæ©ç : [batch, 1, tgt_len]
    tgt_pad_mask = (tgt != pad_idx).unsqueeze(1)
    
    # ç»„åˆï¼š[batch, 1, tgt_len, tgt_len]
    tgt_mask = tgt_causal_mask.unsqueeze(0) & tgt_pad_mask.unsqueeze(2)
    
    return src_mask, tgt_mask


class LabelSmoothing(nn.Module):
    """æ ‡ç­¾å¹³æ»‘"""
    
    def __init__(self, vocab_size, pad_idx, smoothing=0.1):
        super(LabelSmoothing, self).__init__()
        self.criterion = nn.KLDivLoss(reduction='sum')
        self.pad_idx = pad_idx
        self.confidence = 1.0 - smoothing
        self.smoothing = smoothing
        self.vocab_size = vocab_size
    
    def forward(self, x, target):
        """
        x: [batch * seq_len, vocab_size]
        target: [batch * seq_len]
        """
        assert x.size(1) == self.vocab_size
        
        true_dist = torch.zeros_like(x)
        true_dist.fill_(self.smoothing / (self.vocab_size - 2))
        true_dist.scatter_(1, target.unsqueeze(1), self.confidence)
        true_dist[:, self.pad_idx] = 0
        
        mask = torch.nonzero(target == self.pad_idx, as_tuple=False)
        if mask.dim() > 0 and mask.size(0) > 0:
            true_dist.index_fill_(0, mask.squeeze(), 0.0)
        
        return self.criterion(x, true_dist)


def train_epoch(model, dataloader, optimizer, criterion, device, pad_idx, scheduler=None):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0
    total_tokens = 0
    
    pbar = tqdm(dataloader, desc="Training")
    for src, tgt in pbar:
        src, tgt = src.to(device), tgt.to(device)
        
        # å‡†å¤‡decoderè¾“å…¥å’Œç›®æ ‡
        tgt_input = tgt[:, :-1]  # ç§»é™¤æœ€åä¸€ä¸ªtoken
        tgt_output = tgt[:, 1:]  # ç§»é™¤ç¬¬ä¸€ä¸ªtoken (SOS)
        
        # åˆ›å»ºæ©ç 
        src_mask, tgt_mask = create_masks(src, tgt_input, pad_idx)
        
        # å‰å‘ä¼ æ’­
        optimizer.zero_grad()
        output = model(src, tgt_input, src_mask, tgt_mask)
        
        # è®¡ç®—æŸå¤±
        # output: [batch, tgt_len-1, vocab_size]
        # tgt_output: [batch, tgt_len-1]
        output = output.reshape(-1, output.size(-1))
        tgt_output = tgt_output.reshape(-1)
        
        loss = criterion(output.log_softmax(dim=-1), tgt_output)
        
        # åå‘ä¼ æ’­
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        # æ›´æ–°å­¦ä¹ ç‡ï¼ˆåœ¨optimizer.step()ä¹‹å‰è°ƒç”¨ï¼Œç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„å­¦ä¹ ç‡ï¼‰
        if scheduler is not None:
            current_lr = scheduler.step()
        
        # ä¼˜åŒ–å™¨æ›´æ–°å‚æ•°ï¼ˆæ­¤æ—¶ä½¿ç”¨è°ƒåº¦å™¨æ›´æ–°çš„å­¦ä¹ ç‡ï¼‰
        optimizer.step()
        
        # ç»Ÿè®¡
        n_tokens = (tgt_output != pad_idx).sum().item()
        total_loss += loss.item()
        total_tokens += n_tokens
        
        # æ˜¾ç¤ºå½“å‰å­¦ä¹ ç‡
        if scheduler is not None:
            pbar.set_postfix({
                'loss': f'{loss.item() / n_tokens:.4f}',
                'lr': f'{current_lr:.2e}'
            })
        else:
            pbar.set_postfix({'loss': f'{loss.item() / n_tokens:.4f}'})
    
    return total_loss / total_tokens


def evaluate(model, dataloader, criterion, device, pad_idx):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    total_loss = 0
    total_tokens = 0
    
    with torch.no_grad():
        for src, tgt in tqdm(dataloader, desc="Evaluating"):
            src, tgt = src.to(device), tgt.to(device)
            
            tgt_input = tgt[:, :-1]
            tgt_output = tgt[:, 1:]
            
            src_mask, tgt_mask = create_masks(src, tgt_input, pad_idx)
            
            output = model(src, tgt_input, src_mask, tgt_mask)
            
            output = output.reshape(-1, output.size(-1))
            tgt_output = tgt_output.reshape(-1)
            
            loss = criterion(output.log_softmax(dim=-1), tgt_output)
            
            n_tokens = (tgt_output != pad_idx).sum().item()
            total_loss += loss.item()
            total_tokens += n_tokens
    
    return total_loss / total_tokens


def translate(model, src_sentence, src_vocab, tgt_vocab, tokenizer, device, max_len=50):
    """
    ç¿»è¯‘ä¸€ä¸ªå¥å­
    """
    model.eval()
    
    # åˆ†è¯å’Œç¼–ç 
    src_tokens = tokenizer.tokenize(src_sentence)
    src_indices = src_vocab.encode(src_tokens, add_special_tokens=True)
    src = torch.tensor([src_indices]).to(device)
    
    # ç”Ÿæˆ
    with torch.no_grad():
        generated = model.generate(
            src,
            max_len=max_len,
            start_token=tgt_vocab.sos_idx,
            end_token=tgt_vocab.eos_idx
        )
    
    # è§£ç 
    tgt_indices = generated[0].tolist()
    tgt_tokens = tgt_vocab.decode(tgt_indices, skip_special_tokens=True)
    translation = tokenizer.detokenize(tgt_tokens)
    
    return translation


def plot_training_curves(train_losses, val_losses, train_ppls, val_ppls, save_dir):
    """
    ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    Args:
        train_losses: è®­ç»ƒæŸå¤±åˆ—è¡¨
        val_losses: éªŒè¯æŸå¤±åˆ—è¡¨
        train_ppls: è®­ç»ƒå›°æƒ‘åº¦åˆ—è¡¨
        val_ppls: éªŒè¯å›°æƒ‘åº¦åˆ—è¡¨
        save_dir: ä¿å­˜ç›®å½•
    """
    epochs = range(1, len(train_losses) + 1)
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 3.5))
    plt.rcParams.update({'font.size': 10})
    
    # æŸå¤±æ›²çº¿
    ax1.plot(epochs, train_losses, 'b-', label='è®­ç»ƒæŸå¤±', marker='o')
    ax1.plot(epochs, val_losses, 'r-', label='éªŒè¯æŸå¤±', marker='s')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('æŸå¤±')
    ax1.set_title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±æ›²çº¿')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # å›°æƒ‘åº¦æ›²çº¿
    ax2.plot(epochs, train_ppls, 'b-', label='è®­ç»ƒå›°æƒ‘åº¦', marker='o')
    ax2.plot(epochs, val_ppls, 'r-', label='éªŒè¯å›°æƒ‘åº¦', marker='s')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('å›°æƒ‘åº¦')
    ax2.set_title('è®­ç»ƒå’ŒéªŒè¯å›°æƒ‘åº¦æ›²çº¿')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'training_curves.png'), dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"  è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ°: {os.path.join(save_dir, 'training_curves.png')}")


def save_training_log(log_data, save_dir):
    """
    ä¿å­˜è®­ç»ƒæ—¥å¿—ä¸ºCSVå’ŒMarkdownè¡¨æ ¼
    Args:
        log_data: è®­ç»ƒæ—¥å¿—æ•°æ®åˆ—è¡¨
        save_dir: ä¿å­˜ç›®å½•
    """
    df = pd.DataFrame(log_data)
    
    # ä¿å­˜ä¸ºCSV
    csv_path = os.path.join(save_dir, 'training_log.csv')
    df.to_csv(csv_path, index=False, encoding='utf-8')
    print(f"  è®­ç»ƒæ—¥å¿—å·²ä¿å­˜åˆ°: {csv_path}")
    
    # ä¿å­˜ä¸ºMarkdownè¡¨æ ¼
    md_path = os.path.join(save_dir, 'training_log.md')
    with open(md_path, 'w', encoding='utf-8') as f:
        f.write("# è®­ç»ƒæ—¥å¿—\n\n")
        f.write(df.to_markdown(index=False))
        f.write("\n")
    print(f"  Markdownè¡¨æ ¼å·²ä¿å­˜åˆ°: {md_path}")


def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    print("="*60)
    print("Transformer æœºå™¨ç¿»è¯‘è®­ç»ƒ")
    print("="*60)
    
    # åˆ›å»ºç»“æœç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_dir = "results"
    models_dir = os.path.join(results_dir, f"models_{timestamp}")
    plots_dir = os.path.join(results_dir, f"plots_{timestamp}")
    
    os.makedirs(models_dir, exist_ok=True)
    os.makedirs(plots_dir, exist_ok=True)
    
    print(f"\nç»“æœä¿å­˜è·¯å¾„:")
    print(f"  æ¨¡å‹ç›®å½•: {models_dir}")
    print(f"  å›¾è¡¨ç›®å½•: {plots_dir}")
    
    # è¶…å‚æ•°
    config = {
        # æ¨¡å‹æ¶æ„ï¼ˆå·²æ”¹ä¸ºPre-LNï¼Œè®­ç»ƒæ›´ç¨³å®šï¼Œå¯ä»¥é€‚å½“å¢å¤§ï¼‰
        'd_model': 512,  # æ¢å¤åˆ°æ ‡å‡†Transformerçš„d_model
        'num_heads': 8,
        'num_encoder_layers': 6,  # æ¢å¤åˆ°æ ‡å‡†6å±‚ï¼ˆPre-LNæ›´ç¨³å®šï¼‰
        'num_decoder_layers': 6,  # æ¢å¤åˆ°æ ‡å‡†6å±‚
        'd_ff': 2048,  # æ¢å¤åˆ°æ ‡å‡†FFNç»´åº¦ï¼ˆd_model * 4ï¼‰
        'dropout': 0.1,  # é™ä½dropoutï¼ˆPre-LNæ¶æ„è®­ç»ƒæ›´ç¨³å®šï¼Œä¸”å·²ç§»é™¤FFNé¢å¤–dropoutï¼‰
        
        # è®­ç»ƒè¶…å‚æ•°
        'batch_size': 64,  # ä¿æŒå½“å‰batch sizeä»¥é€‚åº”GPUæ˜¾å­˜
        'num_epochs': 30,  # å¢åŠ è®­ç»ƒè½®æ•°ï¼Œæ—©åœä¼šè‡ªåŠ¨ç»ˆæ­¢
        'learning_rate': 0.0001,
        'max_vocab_size': 10000,
        'device': 'cuda:0' if torch.cuda.is_available() else 'cpu',
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨å‚æ•°
        'warmup_steps': 4000,  # æ ‡å‡†warmupæ­¥æ•°ï¼ˆè®ºæ–‡å€¼ï¼‰
        
        # æ—©åœæœºåˆ¶å‚æ•°
        'early_stopping_patience': 10,  # è€å¿ƒå€¼ï¼Œå…è®¸éªŒè¯é›†lossæœ‰10è½®ä¸æ”¹å–„
        'early_stopping_min_delta': 0.001,  # æœ€å°æ”¹å–„é‡
        'early_stopping_enabled': True  # å¯ç”¨æ—©åœ
    }
    
    # ç¡®ä¿åªä½¿ç”¨ä¸€ä¸ªGPUï¼ˆç¬¬ä¸€ä¸ªGPUï¼‰
    if torch.cuda.is_available():
        num_gpus = torch.cuda.device_count()
        if num_gpus > 1:
            print(f"\næ£€æµ‹åˆ° {num_gpus} å—GPUï¼Œä»…ä½¿ç”¨ç¬¬ä¸€å—GPU (cuda:0)")
        print(f"ä½¿ç”¨GPU: {torch.cuda.get_device_name(0)}")
    
    print(f"\né…ç½®:")
    for key, value in config.items():
        print(f"  {key}: {value}")
    
    device = torch.device(config['device'])
    print(f"\nä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºåˆ†è¯å™¨
    print("\nå‡†å¤‡æ•°æ®...")
    tokenizer = SimpleTokenizer(lowercase=False)
    
    # åŠ è½½çœŸå®æ•°æ®é›†ï¼ˆä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼‰
    print("åŠ è½½IWSLT14æ•°æ®é›†...")
    train_src, train_tgt, val_src, val_tgt = load_iwslt14_de_en(
        data_dir="datasets/iwslt14",
        max_samples=None  # ä½¿ç”¨å…¨éƒ¨160kæ•°æ®
    )
    
    print(f"è®­ç»ƒé›†å¤§å°: {len(train_src)}")
    print(f"éªŒè¯é›†å¤§å°: {len(val_src)}")
    
    # åŠ è½½è¯æ±‡è¡¨
    print("\nåŠ è½½è¯æ±‡è¡¨...")
    src_vocab = load_vocabulary_from_file("datasets/iwslt14/vocab.de")
    tgt_vocab = load_vocabulary_from_file("datasets/iwslt14/vocab.en")
    
    print(f"æºè¯­è¨€è¯æ±‡è¡¨å¤§å°: {len(src_vocab)}")
    print(f"ç›®æ ‡è¯­è¨€è¯æ±‡è¡¨å¤§å°: {len(tgt_vocab)}")
    
    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    train_dataset = TranslationDataset(train_src, train_tgt, src_vocab, tgt_vocab, tokenizer)
    val_dataset = TranslationDataset(val_src, val_tgt, src_vocab, tgt_vocab, tokenizer)
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=config['batch_size'],
        shuffle=True,
        collate_fn=lambda batch: collate_fn(batch, pad_idx=src_vocab.pad_idx),
        num_workers=2,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config['batch_size'],
        shuffle=False,
        collate_fn=lambda batch: collate_fn(batch, pad_idx=src_vocab.pad_idx),
        num_workers=2,
        pin_memory=True
    )
    
    # åˆ›å»ºæ¨¡å‹
    print("\nåˆ›å»ºTransformeræ¨¡å‹...")
    model = Transformer(
        src_vocab_size=len(src_vocab),
        tgt_vocab_size=len(tgt_vocab),
        d_model=config['d_model'],
        num_heads=config['num_heads'],
        num_encoder_layers=config['num_encoder_layers'],
        num_decoder_layers=config['num_decoder_layers'],
        d_ff=config['d_ff'],
        dropout=config['dropout'],
        pad_idx=src_vocab.pad_idx  # ğŸ”§ ä¿®å¤: ä¼ é€’paddingç´¢å¼•
    ).to(device)
    
    # ç»Ÿè®¡å‚æ•°
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"æ€»å‚æ•°é‡: {total_params:,}")
    print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    # é™ä½æ ‡ç­¾å¹³æ»‘ä»¥é¿å…è¿‡åº¦æ­£åˆ™åŒ–ï¼ˆ0.1 -> 0.05ï¼‰
    criterion = LabelSmoothing(
        vocab_size=len(tgt_vocab),
        pad_idx=tgt_vocab.pad_idx,
        smoothing=0.05
    )
    
    # ä¼˜åŒ–å™¨ï¼šä½¿ç”¨è¾ƒå°çš„åˆå§‹å­¦ä¹ ç‡ï¼ˆNoamè°ƒåº¦å™¨ä¼šè‡ªåŠ¨è°ƒæ•´ï¼‰
    # æ ¹æ®è®ºæ–‡ï¼Œåˆå§‹å­¦ä¹ ç‡è®¾ç½®ä¸º1.0ï¼ˆä½†å®é™…ä¼šç”±è°ƒåº¦å™¨ç®¡ç†ï¼‰
    optimizer = optim.Adam(
        model.parameters(),
        lr=1.0,  # Noamè°ƒåº¦å™¨ä¼šåŠ¨æ€è°ƒæ•´ï¼Œè¿™é‡Œè®¾ç½®ä¸º1.0ä½œä¸ºåŸºå‡†
        betas=(0.9, 0.98),
        eps=1e-9
    )
    
    # åˆ›å»ºNoamå­¦ä¹ ç‡è°ƒåº¦å™¨
    # æ ¹æ®æ•°æ®é›†å¤§å°è°ƒæ•´ warmup_steps
    # å¯¹äºå¤§æ•°æ®é›†ï¼Œå»ºè®® warmup çº¦å æ€»è®­ç»ƒçš„ 5-10%
    warmup_steps = config.get('warmup_steps', 8000)  # å¢åŠ åˆ°8000æ­¥ï¼ˆçº¦3.2ä¸ªepochï¼‰
    scheduler = NoamScheduler(
        optimizer=optimizer,
        d_model=config['d_model'],
        warmup_steps=warmup_steps,
        factor=1.0
    )
    # ç«‹å³è®¾ç½®åˆå§‹å­¦ä¹ ç‡åˆ°optimizerï¼ˆåœ¨ç¬¬ä¸€æ¬¡è®­ç»ƒè¿­ä»£ä¹‹å‰ï¼‰
    # ç¬¬ä¸€æ¬¡è°ƒç”¨scheduler.step()æ—¶_stepä¼šä»0å˜æˆ1ï¼Œæ‰€ä»¥æˆ‘ä»¬é¢„å…ˆè®¾ç½®step=1çš„å­¦ä¹ ç‡
    initial_lr = scheduler._calculate_rate(1)
    for param_group in optimizer.param_groups:
        param_group['lr'] = initial_lr
    print(f"\nä½¿ç”¨Noamå­¦ä¹ ç‡è°ƒåº¦å™¨ (warmup_steps={warmup_steps})")
    print(f"åˆå§‹å­¦ä¹ ç‡ (step=1): {initial_lr:.2e} (å·²é¢„å…ˆè®¾ç½®åˆ°optimizer)")
    
    # è®­ç»ƒ
    print("\n" + "="*60)
    print("å¼€å§‹è®­ç»ƒ")
    print("="*60)
    
    best_val_loss = float('inf')
    train_losses = []
    val_losses = []
    train_ppls = []
    val_ppls = []
    log_data = []
    
    # æ—©åœæœºåˆ¶ç›¸å…³å˜é‡
    early_stopping_patience = config.get('early_stopping_patience', 10)
    early_stopping_min_delta = config.get('early_stopping_min_delta', 0.001)
    early_stopping_enabled = config.get('early_stopping_enabled', True)
    patience_counter = 0  # è¿ç»­æ²¡æœ‰æ”¹å–„çš„epochæ•°
    best_epoch = 0  # æœ€ä½³éªŒè¯æŸå¤±çš„epoch
    
    if early_stopping_enabled:
        print(f"\næ—©åœæœºåˆ¶å·²å¯ç”¨:")
        print(f"  è€å¿ƒå€¼ (patience): {early_stopping_patience}")
        print(f"  æœ€å°æ”¹å–„é‡ (min_delta): {early_stopping_min_delta}")
    
    for epoch in range(config['num_epochs']):
        start_time = time.time()
        
        print(f"\nEpoch {epoch + 1}/{config['num_epochs']}")
        
        # è®­ç»ƒ
        train_loss = train_epoch(model, train_loader, optimizer, criterion, device, src_vocab.pad_idx, scheduler)
        
        # éªŒè¯
        val_loss = evaluate(model, val_loader, criterion, device, src_vocab.pad_idx)
        
        end_time = time.time()
        epoch_time = end_time - start_time
        
        # è®¡ç®—å›°æƒ‘åº¦
        train_ppl = math.exp(train_loss) if train_loss < 10 else float('inf')
        val_ppl = math.exp(val_loss) if val_loss < 10 else float('inf')
        
        # è®°å½•æ•°æ®
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        train_ppls.append(train_ppl)
        val_ppls.append(val_ppl)
        
        # è·å–å½“å‰å­¦ä¹ ç‡
        current_lr = scheduler.get_lr()
        
        log_data.append({
            'Epoch': epoch + 1,
            'è®­ç»ƒæŸå¤±': f'{train_loss:.4f}',
            'éªŒè¯æŸå¤±': f'{val_loss:.4f}',
            'è®­ç»ƒå›°æƒ‘åº¦': f'{train_ppl:.2f}',
            'éªŒè¯å›°æƒ‘åº¦': f'{val_ppl:.2f}',
            'å­¦ä¹ ç‡': f'{current_lr:.2e}',
            'ç”¨æ—¶(ç§’)': f'{epoch_time:.2f}'
        })
        
        print(f"\nEpoch {epoch + 1} å®Œæˆ:")
        print(f"  è®­ç»ƒæŸå¤±: {train_loss:.4f} | è®­ç»ƒå›°æƒ‘åº¦: {train_ppl:.2f}")
        print(f"  éªŒè¯æŸå¤±: {val_loss:.4f} | éªŒè¯å›°æƒ‘åº¦: {val_ppl:.2f}")
        print(f"  å­¦ä¹ ç‡: {current_lr:.2e}")
        print(f"  ç”¨æ—¶: {epoch_time:.2f}ç§’")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹å’Œæ—©åœåˆ¤æ–­
        if val_loss < best_val_loss - early_stopping_min_delta:
            # éªŒè¯æŸå¤±æœ‰æ˜æ˜¾æ”¹å–„
            best_val_loss = val_loss
            best_epoch = epoch + 1
            patience_counter = 0  # é‡ç½®è®¡æ•°å™¨
            
            model_path = os.path.join(models_dir, 'best_model.pt')
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'val_loss': val_loss,
                'src_vocab': src_vocab,
                'tgt_vocab': tgt_vocab,
                'config': config,
            }, model_path)
            print(f"  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹åˆ°: {model_path} (éªŒè¯æŸå¤±æ”¹å–„: {best_val_loss:.4f})")
        else:
            # éªŒè¯æŸå¤±æ²¡æœ‰æ”¹å–„
            if early_stopping_enabled:
                patience_counter += 1
                print(f"  éªŒè¯æŸå¤±æœªæ”¹å–„ (è€å¿ƒè®¡æ•°: {patience_counter}/{early_stopping_patience})")
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ—©åœ
                if patience_counter >= early_stopping_patience:
                    print(f"\n{'='*60}")
                    print("è§¦å‘æ—©åœæœºåˆ¶")
                    print(f"{'='*60}")
                    print(f"éªŒè¯æŸå¤±å·²è¿ç»­ {early_stopping_patience} ä¸ªepochæœªæ”¹å–„")
                    print(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f} (Epoch {best_epoch})")
                    print(f"å½“å‰éªŒè¯æŸå¤±: {val_loss:.4f}")
                    print(f"è®­ç»ƒå·²åœæ­¢åœ¨ç¬¬ {epoch + 1} ä¸ªepoch")
                    break
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % 5 == 0 or (epoch + 1) == config['num_epochs']:
            checkpoint_path = os.path.join(models_dir, f'checkpoint_epoch_{epoch+1}.pt')
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'train_loss': train_loss,
                'val_loss': val_loss,
                'config': config,
            }, checkpoint_path)
            print(f"  âœ“ ä¿å­˜æ£€æŸ¥ç‚¹åˆ°: {checkpoint_path}")
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    print("\nç”Ÿæˆè®­ç»ƒæ›²çº¿...")
    plot_training_curves(train_losses, val_losses, train_ppls, val_ppls, plots_dir)
    
    # ä¿å­˜è®­ç»ƒæ—¥å¿—
    print("\nä¿å­˜è®­ç»ƒæ—¥å¿—...")
    save_training_log(log_data, plots_dir)
    
    print("\n" + "="*60)
    print("è®­ç»ƒå®Œæˆï¼")
    print("="*60)
    print(f"æ€»è®­ç»ƒè½®æ•°: {len(train_losses)}/{config['num_epochs']}")
    if early_stopping_enabled and patience_counter >= early_stopping_patience:
        print(f"è®­ç»ƒå› æ—©åœæœºåˆ¶è€Œç»“æŸ")
    print(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f} (Epoch {best_epoch})")
    if len(val_losses) > 0:
        print(f"æœ€ç»ˆéªŒè¯æŸå¤±: {val_losses[-1]:.4f}")
    print(f"\nç»“æœä¿å­˜ä½ç½®:")
    print(f"  æ¨¡å‹: {models_dir}")
    print(f"  å›¾è¡¨å’Œæ—¥å¿—: {plots_dir}")


if __name__ == "__main__":
    main()

